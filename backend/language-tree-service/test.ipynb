{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fafdb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Relationship Extraction Pipeline\n",
      "==================================================\n",
      "\n",
      "Key Wikidata Properties for Language Relationships:\n",
      "  instance_of: P31\n",
      "  subclass_of: P279\n",
      "  language_family: P25295\n",
      "  parent_language: P155\n",
      "  child_language: P156\n",
      "  dialect_of: P629\n",
      "  has_dialect: P2341\n",
      "  related_language: P2596\n",
      "  writing_system: P282\n",
      "  iso_639_1: P218\n",
      "  iso_639_2: P219\n",
      "  iso_639_3: P220\n",
      "  glottolog_id: P1394\n",
      "\n",
      "Key Language Classes in Wikidata:\n",
      "  language: Q34770\n",
      "  natural_language: Q33742\n",
      "  constructed_language: Q33215\n",
      "  dead_language: Q45762\n",
      "  extinct_language: Q45762\n",
      "  language_family: Q25295\n",
      "  language_isolate: Q34770\n",
      "  dialect: Q33384\n",
      "  variety_of_language: Q33384\n",
      "\n",
      "==================================================\n",
      "EXAMPLE EXTRACTION\n",
      "==================================================\n",
      "Example: Extracting relationships for 'Spanish'\n",
      "--------------------------------------------------\n",
      "Found Wikidata entity: Q1860\n",
      "\n",
      "Relationships found:\n",
      "{\n",
      "  \"genetic_descent\": {\n",
      "    \"parents\": [],\n",
      "    \"children\": [],\n",
      "    \"ancestors\": [],\n",
      "    \"descendants\": []\n",
      "  },\n",
      "  \"dialects\": {\n",
      "    \"dialect_of\": [],\n",
      "    \"has_dialects\": [\n",
      "      {\n",
      "        \"id\": \"Q21\",\n",
      "        \"label\": \"England\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"siblings\": [],\n",
      "  \"language_family\": [\n",
      "    {\n",
      "      \"id\": \"eng\",\n",
      "      \"label\": \"eng\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"P31\": {\n",
      "      \"id\": \"Q33742\",\n",
      "      \"label\": \"natural language\"\n",
      "    },\n",
      "    \"P279\": {\n",
      "      \"id\": \"Q1346342\",\n",
      "      \"label\": \"Anglic\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's create the pipeline without SPARQLWrapper first, then show how to install it\n",
    "# This version will use direct HTTP requests to the SPARQL endpoint\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "class LanguageRelationshipExtractor:\n",
    "    \"\"\"\n",
    "    A comprehensive pipeline for extracting language relationships from Wikipedia/Wikidata.\n",
    "    Supports genetic descent edges, dialect edges, and sibling relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize endpoints and headers\n",
    "        self.wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
    "        self.wikipedia_api = \"https://en.wikipedia.org/w/api.php\"\n",
    "        self.sparql_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        # Common headers for API requests\n",
    "        self.headers = {\n",
    "            'User-Agent': 'LanguageRelationshipExtractor/1.0 (https://example.com/contact)',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        \n",
    "        # Key Wikidata properties for language relationships\n",
    "        self.language_properties = {\n",
    "            'instance_of': 'P31',           # Instance of\n",
    "            'subclass_of': 'P279',          # Subclass of  \n",
    "            'language_family': 'P220',      # Language family\n",
    "            'parent_language': 'P155',      # Follows (parent language)\n",
    "            'child_language': 'P156',       # Followed by (child language)\n",
    "            'dialect_of': 'P629',           # Dialect of\n",
    "            'has_dialect': 'P2341',         # Has dialect\n",
    "            'related_language': 'P2596',    # Related language\n",
    "            'writing_system': 'P282',       # Writing system used\n",
    "            'iso_639_1': 'P218',           # ISO 639-1 code\n",
    "            'iso_639_2': 'P219',           # ISO 639-2 code\n",
    "            'iso_639_3': 'P220',           # ISO 639-3 code (Note: this overwrites language_family, we'll fix this)\n",
    "            'glottolog_id': 'P1394',       # Glottolog ID\n",
    "        }\n",
    "        \n",
    "        # Fix the ISO code mapping\n",
    "        self.language_properties['iso_639_3'] = 'P220'\n",
    "        self.language_properties['language_family'] = 'P25295'  # Corrected\n",
    "        \n",
    "        # Language-related classes in Wikidata\n",
    "        self.language_classes = {\n",
    "            'language': 'Q34770',\n",
    "            'natural_language': 'Q33742',\n",
    "            'constructed_language': 'Q33215',\n",
    "            'dead_language': 'Q45762',\n",
    "            'extinct_language': 'Q45762',\n",
    "            'language_family': 'Q25295',\n",
    "            'language_isolate': 'Q34770',\n",
    "            'dialect': 'Q33384',\n",
    "            'variety_of_language': 'Q33384'\n",
    "        }\n",
    "    \n",
    "    def execute_sparql_query(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute SPARQL query using direct HTTP requests.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'query': query,\n",
    "                'format': 'json'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(\n",
    "                self.sparql_endpoint, \n",
    "                params=params, \n",
    "                headers=self.headers,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error executing SPARQL query: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def get_wikidata_entity_by_name(self, language_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get Wikidata entity ID (Q-number) for a language by name.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': language_name,\n",
    "                'type': 'item',\n",
    "                'limit': 10\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikidata_api, params=params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Look for exact matches or language-related items\n",
    "            for item in data.get('search', []):\n",
    "                description = item.get('description', '').lower()\n",
    "                if any(term in description for term in ['language', 'dialect', 'tongue']):\n",
    "                    return item['id']\n",
    "            \n",
    "            # If no language-specific match, return first result\n",
    "            if data.get('search'):\n",
    "                return data['search'][0]['id']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for entity {language_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_language_relationships_sparql(self, entity_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract language relationships using SPARQL queries.\n",
    "        \"\"\"\n",
    "        relationships = {\n",
    "            'genetic_descent': {\n",
    "                'parents': [],\n",
    "                'children': [],\n",
    "                'ancestors': [],\n",
    "                'descendants': []\n",
    "            },\n",
    "            'dialects': {\n",
    "                'dialect_of': [],\n",
    "                'has_dialects': []\n",
    "            },\n",
    "            'siblings': [],\n",
    "            'language_family': [],\n",
    "            'metadata': {}\n",
    "        }\n",
    "        \n",
    "        # Query 1: Basic language information and direct relationships\n",
    "        basic_query = f\"\"\"\n",
    "        SELECT DISTINCT ?prop ?propLabel ?value ?valueLabel WHERE {{\n",
    "            wd:{entity_id} ?prop ?value .\n",
    "            ?property wikibase:directClaim ?prop .\n",
    "            FILTER(?prop IN (wdt:P31, wdt:P279, wdt:P220, wdt:P155, wdt:P156, wdt:P629, wdt:P2341, wdt:P2596))\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            results = self.execute_sparql_query(basic_query)\n",
    "            \n",
    "            for result in results.get(\"results\", {}).get(\"bindings\", []):\n",
    "                prop = result[\"prop\"][\"value\"].split('/')[-1]\n",
    "                value_id = result[\"value\"][\"value\"].split('/')[-1]\n",
    "                value_label = result.get(\"valueLabel\", {}).get(\"value\", value_id)\n",
    "                \n",
    "                # Map properties to relationship types\n",
    "                if prop == 'P155':  # Parent language (follows)\n",
    "                    relationships['genetic_descent']['parents'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P156':  # Child language (followed by)\n",
    "                    relationships['genetic_descent']['children'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P629':  # Dialect of\n",
    "                    relationships['dialects']['dialect_of'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P2341':  # Has dialect\n",
    "                    relationships['dialects']['has_dialects'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P220':  # Language family\n",
    "                    relationships['language_family'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P31' or prop == 'P279':  # Instance of / Subclass of\n",
    "                    relationships['metadata'][prop] = {\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    }\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in basic SPARQL query: {e}\")\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def get_wikipedia_infobox_data(self, language_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract language relationship data from Wikipedia infoboxes.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, find the Wikipedia page\n",
    "            search_params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'list': 'search',\n",
    "                'srsearch': f\"{language_name} language\",\n",
    "                'srnamespace': 0,\n",
    "                'srlimit': 5\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikipedia_api, params=search_params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            search_data = response.json()\n",
    "            \n",
    "            if not search_data.get('query', {}).get('search'):\n",
    "                return {}\n",
    "            \n",
    "            # Get the page content\n",
    "            page_title = search_data['query']['search'][0]['title']\n",
    "            content_params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'titles': page_title,\n",
    "                'prop': 'revisions',\n",
    "                'rvprop': 'content',\n",
    "                'rvslots': 'main'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikipedia_api, params=content_params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            content_data = response.json()\n",
    "            \n",
    "            # Extract infobox data\n",
    "            pages = content_data.get('query', {}).get('pages', {})\n",
    "            for page_id, page_data in pages.items():\n",
    "                if 'revisions' in page_data:\n",
    "                    content = page_data['revisions'][0]['slots']['main']['*']\n",
    "                    return self._parse_language_infobox(content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting Wikipedia infobox data: {e}\")\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _parse_language_infobox(self, content: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse language infobox content to extract relationships.\n",
    "        \"\"\"\n",
    "        relationships = {\n",
    "            'family': [],\n",
    "            'ancestors': [],\n",
    "            'dialects': [],\n",
    "            'related': []\n",
    "        }\n",
    "        \n",
    "        # Look for infobox language or language family templates\n",
    "        infobox_patterns = [\n",
    "            r'\\{\\{Infobox language(.*?)\\}\\}',\n",
    "            r'\\{\\{Infobox language family(.*?)\\}\\}'\n",
    "        ]\n",
    "        \n",
    "        for pattern in infobox_patterns:\n",
    "            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                # Extract family information\n",
    "                family_matches = re.findall(r'fam\\d+\\s*=\\s*\\[\\[(.*?)\\]\\]', match)\n",
    "                relationships['family'].extend(family_matches)\n",
    "                \n",
    "                # Extract ancestor information\n",
    "                ancestor_matches = re.findall(r'ancestor\\d*\\s*=\\s*\\[\\[(.*?)\\]\\]', match)\n",
    "                relationships['ancestors'].extend(ancestor_matches)\n",
    "                \n",
    "                # Extract dialect information\n",
    "                dialect_matches = re.findall(r'dia\\d+\\s*=\\s*\\[\\[(.*?)\\]\\]', match)\n",
    "                relationships['dialects'].extend(dialect_matches)\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "# Create a demonstration function\n",
    "def create_example_extraction():\n",
    "    \"\"\"\n",
    "    Create an example extraction for demonstration purposes.\n",
    "    \"\"\"\n",
    "    extractor = LanguageRelationshipExtractor()\n",
    "    \n",
    "    # Example: Extract relationships for Spanish\n",
    "    print(\"Example: Extracting relationships for 'Spanish'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Step 1: Get Wikidata entity\n",
    "    entity_id = extractor.get_wikidata_entity_by_name(\"English\")\n",
    "    if entity_id:\n",
    "        print(f\"Found Wikidata entity: {entity_id}\")\n",
    "        \n",
    "        # Step 2: Get basic relationships via SPARQL\n",
    "        relationships = extractor.get_language_relationships_sparql(entity_id)\n",
    "        \n",
    "        print(\"\\nRelationships found:\")\n",
    "        print(json.dumps(relationships, indent=2))\n",
    "        \n",
    "        return relationships\n",
    "    else:\n",
    "        print(\"Could not find entity for Spanish\")\n",
    "        return {}\n",
    "\n",
    "# Initialize the pipeline\n",
    "print(\"Language Relationship Extraction Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "extractor = LanguageRelationshipExtractor()\n",
    "\n",
    "print(\"\\nKey Wikidata Properties for Language Relationships:\")\n",
    "for name, prop_id in extractor.language_properties.items():\n",
    "    print(f\"  {name}: {prop_id}\")\n",
    "\n",
    "print(\"\\nKey Language Classes in Wikidata:\")\n",
    "for name, class_id in extractor.language_classes.items():\n",
    "    print(f\"  {name}: {class_id}\")\n",
    "\n",
    "# Run example\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXAMPLE EXTRACTION\")\n",
    "print(\"=\" * 50)\n",
    "example_result = create_example_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea880d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Relationship Extraction Pipeline\n",
      "==================================================\n",
      "\n",
      "Key Wikidata Properties for Language Relationships:\n",
      "  instance_of: P31\n",
      "  subclass_of: P279\n",
      "  language_family: P220\n",
      "  parent_language: P155\n",
      "  child_language: P156\n",
      "  dialect_of: P629\n",
      "  has_dialect: P2341\n",
      "  related_language: P2596\n",
      "  writing_system: P282\n",
      "  iso_639_1: P218\n",
      "  iso_639_2: P219\n",
      "  iso_639_3: P220\n",
      "  glottolog_id: P1394\n",
      "\n",
      "Key Language Classes in Wikidata:\n",
      "  language: Q34770\n",
      "  natural_language: Q33742\n",
      "  constructed_language: Q33215\n",
      "  dead_language: Q45762\n",
      "  extinct_language: Q45762\n",
      "  language_family: Q25295\n",
      "  language_isolate: Q34770\n",
      "  dialect: Q33384\n",
      "  variety_of_language: Q33384\n",
      "Extracting relationships for: English\n",
      "Found Wikidata entity: Q1860\n",
      "Extracting via SPARQL...\n",
      "Found Wikidata entity: Q1860\n",
      "Extracting via SPARQL...\n",
      "Extracting from Wikipedia infobox...\n",
      "Extracting from Wikipedia infobox...\n",
      "Extracting via Wikidata API...\n",
      "Extracting via Wikidata API...\n",
      "{'entity_id': 'Q1860', 'language_name': 'English', 'sparql_data': {'genetic_descent': {'parents': [], 'children': [], 'ancestors': [], 'descendants': []}, 'dialects': {'dialect_of': [], 'has_dialects': [{'id': 'Q21', 'label': 'England'}]}, 'siblings': [], 'language_family': [{'id': 'eng', 'label': 'eng'}], 'metadata': {'P31': {'id': 'Q33742', 'label': 'natural language'}, 'P279': {'id': 'Q1346342', 'label': 'Anglic'}}}, 'wikipedia_infobox': {'family': [], 'ancestors': [], 'dialects': [], 'related': []}, 'wikidata_api': {'instance_of': ['Q33742', 'Q1288568', 'Q34770', 'Q152559', 'Q399495', 'Q1097949', 'Q20162172'], 'subclass_of': ['Q1346342', 'Q5329170'], 'language_family': ['eng'], 'has_dialect': ['Q21'], 'writing_system': ['Q8229', 'Q3491268'], 'iso_639_1': ['en'], 'iso_639_2': ['eng'], 'iso_639_3': ['eng'], 'glottolog_id': ['stan1293']}, 'combined_relationships': {'genetic_descent': {'parents': [], 'children': [], 'ancestors': [], 'descendants': []}, 'dialects': {'dialect_of': [], 'has_dialects': [{'id': 'Q21', 'label': 'England'}]}, 'siblings': [], 'language_family': [{'id': 'eng', 'label': 'eng'}]}}\n",
      "\n",
      "Pipeline created successfully!\n",
      "Ready to extract language relationships.\n",
      "{English, has_dialect, England}\n",
      "{English, language_family, eng}\n",
      "{'entity_id': 'Q1860', 'language_name': 'English', 'sparql_data': {'genetic_descent': {'parents': [], 'children': [], 'ancestors': [], 'descendants': []}, 'dialects': {'dialect_of': [], 'has_dialects': [{'id': 'Q21', 'label': 'England'}]}, 'siblings': [], 'language_family': [{'id': 'eng', 'label': 'eng'}], 'metadata': {'P31': {'id': 'Q33742', 'label': 'natural language'}, 'P279': {'id': 'Q1346342', 'label': 'Anglic'}}}, 'wikipedia_infobox': {'family': [], 'ancestors': [], 'dialects': [], 'related': []}, 'wikidata_api': {'instance_of': ['Q33742', 'Q1288568', 'Q34770', 'Q152559', 'Q399495', 'Q1097949', 'Q20162172'], 'subclass_of': ['Q1346342', 'Q5329170'], 'language_family': ['eng'], 'has_dialect': ['Q21'], 'writing_system': ['Q8229', 'Q3491268'], 'iso_639_1': ['en'], 'iso_639_2': ['eng'], 'iso_639_3': ['eng'], 'glottolog_id': ['stan1293']}, 'combined_relationships': {'genetic_descent': {'parents': [], 'children': [], 'ancestors': [], 'descendants': []}, 'dialects': {'dialect_of': [], 'has_dialects': [{'id': 'Q21', 'label': 'England'}]}, 'siblings': [], 'language_family': [{'id': 'eng', 'label': 'eng'}]}}\n",
      "\n",
      "Pipeline created successfully!\n",
      "Ready to extract language relationships.\n",
      "{English, has_dialect, England}\n",
      "{English, language_family, eng}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'language_Name1': 'English',\n",
       "  'relationship': 'has_dialect',\n",
       "  'Language_Name2': 'England'},\n",
       " {'language_Name1': 'English',\n",
       "  'relationship': 'language_family',\n",
       "  'Language_Name2': 'eng'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a comprehensive Python pipeline for extracting language relationships from Wikipedia/Wikidata\n",
    "# This will include all the methods mentioned and create a complete solution\n",
    "\n",
    "import requests\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "class LanguageRelationshipExtractor:\n",
    "    \"\"\"\n",
    "    A comprehensive pipeline for extracting language relationships from Wikipedia/Wikidata.\n",
    "    Supports genetic descent edges, dialect edges, and sibling relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize endpoints and headers\n",
    "        self.wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
    "        self.wikipedia_api = \"https://en.wikipedia.org/w/api.php\"\n",
    "        self.sparql_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "        # Initialize SPARQL wrapper\n",
    "        self.sparql = SPARQLWrapper(self.sparql_endpoint)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "        \n",
    "        # Common headers for API requests\n",
    "        self.headers = {\n",
    "            'User-Agent': 'LanguageRelationshipExtractor/1.0 (https://example.com/contact)'\n",
    "        }\n",
    "        \n",
    "        # Key Wikidata properties for language relationships\n",
    "        self.language_properties = {\n",
    "            'instance_of': 'P31',           # Instance of\n",
    "            'subclass_of': 'P279',          # Subclass of  \n",
    "            'language_family': 'P220',      # Language family\n",
    "            'parent_language': 'P155',      # Follows (parent language)\n",
    "            'child_language': 'P156',       # Followed by (child language)\n",
    "            'dialect_of': 'P629',           # Dialect of\n",
    "            'has_dialect': 'P2341',         # Has dialect\n",
    "            'related_language': 'P2596',    # Related language\n",
    "            'writing_system': 'P282',       # Writing system used\n",
    "            'iso_639_1': 'P218',           # ISO 639-1 code\n",
    "            'iso_639_2': 'P219',           # ISO 639-2 code\n",
    "            'iso_639_3': 'P220',           # ISO 639-3 code\n",
    "            'glottolog_id': 'P1394',       # Glottolog ID\n",
    "        }\n",
    "        \n",
    "        # Language-related classes in Wikidata\n",
    "        self.language_classes = {\n",
    "            'language': 'Q34770',\n",
    "            'natural_language': 'Q33742',\n",
    "            'constructed_language': 'Q33215',\n",
    "            'dead_language': 'Q45762',\n",
    "            'extinct_language': 'Q45762',\n",
    "            'language_family': 'Q25295',\n",
    "            'language_isolate': 'Q34770',\n",
    "            'dialect': 'Q33384',\n",
    "            'variety_of_language': 'Q33384'\n",
    "        }\n",
    "    \n",
    "    def get_wikidata_entity_by_name(self, language_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get Wikidata entity ID (Q-number) for a language by name.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': language_name,\n",
    "                'type': 'item',\n",
    "                'limit': 10\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikidata_api, params=params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Look for exact matches or language-related items\n",
    "            for item in data.get('search', []):\n",
    "                description = item.get('description', '').lower()\n",
    "                if any(term in description for term in ['language', 'dialect', 'tongue']):\n",
    "                    return item['id']\n",
    "            \n",
    "            # If no language-specific match, return first result\n",
    "            if data.get('search'):\n",
    "                return data['search'][0]['id']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for entity {language_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_language_relationships_sparql(self, entity_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract language relationships using SPARQL queries.\n",
    "        \"\"\"\n",
    "        relationships = {\n",
    "            'genetic_descent': {\n",
    "                'parents': [],\n",
    "                'children': [],\n",
    "                'ancestors': [],\n",
    "                'descendants': []\n",
    "            },\n",
    "            'dialects': {\n",
    "                'dialect_of': [],\n",
    "                'has_dialects': []\n",
    "            },\n",
    "            'siblings': [],\n",
    "            'language_family': [],\n",
    "            'metadata': {}\n",
    "        }\n",
    "        \n",
    "        # Query 1: Basic language information and direct relationships\n",
    "        basic_query = f\"\"\"\n",
    "        SELECT DISTINCT ?prop ?propLabel ?value ?valueLabel WHERE {{\n",
    "            wd:{entity_id} ?prop ?value .\n",
    "            ?property wikibase:directClaim ?prop .\n",
    "            FILTER(?prop IN (wdt:P31, wdt:P279, wdt:P220, wdt:P155, wdt:P156, wdt:P629, wdt:P2341, wdt:P2596))\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.sparql.setQuery(basic_query)\n",
    "            results = self.sparql.query().convert()\n",
    "            \n",
    "            for result in results[\"results\"][\"bindings\"]:\n",
    "                prop = result[\"prop\"][\"value\"].split('/')[-1]\n",
    "                value_id = result[\"value\"][\"value\"].split('/')[-1]\n",
    "                value_label = result.get(\"valueLabel\", {}).get(\"value\", value_id)\n",
    "                \n",
    "                # Map properties to relationship types\n",
    "                if prop == 'P155':  # Parent language (follows)\n",
    "                    relationships['genetic_descent']['parents'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P156':  # Child language (followed by)\n",
    "                    relationships['genetic_descent']['children'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P629':  # Dialect of\n",
    "                    relationships['dialects']['dialect_of'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P2341':  # Has dialect\n",
    "                    relationships['dialects']['has_dialects'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P220':  # Language family\n",
    "                    relationships['language_family'].append({\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    })\n",
    "                elif prop == 'P31' or prop == 'P279':  # Instance of / Subclass of\n",
    "                    relationships['metadata'][prop] = {\n",
    "                        'id': value_id, 'label': value_label\n",
    "                    }\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in basic SPARQL query: {e}\")\n",
    "        \n",
    "        # Query 2: Find sibling languages (same language family)\n",
    "        if relationships['language_family']:\n",
    "            family_id = relationships['language_family'][0]['id']\n",
    "            sibling_query = f\"\"\"\n",
    "            SELECT DISTINCT ?sibling ?siblingLabel WHERE {{\n",
    "                ?sibling wdt:P220 wd:{family_id} .\n",
    "                ?sibling wdt:P31/wdt:P279* wd:Q34770 .\n",
    "                FILTER(?sibling != wd:{entity_id})\n",
    "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "            }}\n",
    "            LIMIT 20\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                self.sparql.setQuery(sibling_query)\n",
    "                results = self.sparql.query().convert()\n",
    "                \n",
    "                for result in results[\"results\"][\"bindings\"]:\n",
    "                    sibling_id = result[\"sibling\"][\"value\"].split('/')[-1]\n",
    "                    sibling_label = result.get(\"siblingLabel\", {}).get(\"value\", sibling_id)\n",
    "                    relationships['siblings'].append({\n",
    "                        'id': sibling_id, 'label': sibling_label\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in sibling SPARQL query: {e}\")\n",
    "        \n",
    "        # Query 3: Transitive ancestors and descendants\n",
    "        ancestor_query = f\"\"\"\n",
    "        SELECT DISTINCT ?ancestor ?ancestorLabel WHERE {{\n",
    "            wd:{entity_id} wdt:P155+ ?ancestor .\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        descendant_query = f\"\"\"\n",
    "        SELECT DISTINCT ?descendant ?descendantLabel WHERE {{\n",
    "            ?descendant wdt:P155+ wd:{entity_id} .\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute ancestor query\n",
    "        try:\n",
    "            self.sparql.setQuery(ancestor_query)\n",
    "            results = self.sparql.query().convert()\n",
    "            \n",
    "            for result in results[\"results\"][\"bindings\"]:\n",
    "                ancestor_id = result[\"ancestor\"][\"value\"].split('/')[-1]\n",
    "                ancestor_label = result.get(\"ancestorLabel\", {}).get(\"value\", ancestor_id)\n",
    "                relationships['genetic_descent']['ancestors'].append({\n",
    "                    'id': ancestor_id, 'label': ancestor_label\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error in ancestor SPARQL query: {e}\")\n",
    "        \n",
    "        # Execute descendant query\n",
    "        try:\n",
    "            self.sparql.setQuery(descendant_query)\n",
    "            results = self.sparql.query().convert()\n",
    "            \n",
    "            for result in results[\"results\"][\"bindings\"]:\n",
    "                descendant_id = result[\"descendant\"][\"value\"].split('/')[-1]\n",
    "                descendant_label = result.get(\"descendantLabel\", {}).get(\"value\", descendant_id)\n",
    "                relationships['genetic_descent']['descendants'].append({\n",
    "                    'id': descendant_id, 'label': descendant_label\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error in descendant SPARQL query: {e}\")\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def get_wikipedia_infobox_data(self, language_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract language relationship data from Wikipedia infoboxes.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, find the Wikipedia page\n",
    "            search_params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'list': 'search',\n",
    "                'srsearch': f\"{language_name} language\",\n",
    "                'srnamespace': 0,\n",
    "                'srlimit': 5\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikipedia_api, params=search_params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            search_data = response.json()\n",
    "            \n",
    "            if not search_data.get('query', {}).get('search'):\n",
    "                return {}\n",
    "            \n",
    "            # Get the page content\n",
    "            page_title = search_data['query']['search'][0]['title']\n",
    "            content_params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'titles': page_title,\n",
    "                'prop': 'revisions',\n",
    "                'rvprop': 'content',\n",
    "                'rvslots': 'main'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikipedia_api, params=content_params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            content_data = response.json()\n",
    "            \n",
    "            # Extract infobox data\n",
    "            pages = content_data.get('query', {}).get('pages', {})\n",
    "            for page_id, page_data in pages.items():\n",
    "                if 'revisions' in page_data:\n",
    "                    content = page_data['revisions'][0]['slots']['main']['*']\n",
    "                    return self._parse_language_infobox(content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting Wikipedia infobox data: {e}\")\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _parse_language_infobox(self, content: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse language infobox content to extract relationships.\n",
    "        \"\"\"\n",
    "        relationships = {\n",
    "            'family': [],\n",
    "            'ancestors': [],\n",
    "            'dialects': [],\n",
    "            'related': []\n",
    "        }\n",
    "        \n",
    "        # Look for infobox language or language family templates\n",
    "        infobox_patterns = [\n",
    "            r'\\{\\{Infobox language(.*?)\\}\\}',\n",
    "            r'\\{\\{Infobox language family(.*?)\\}\\}'\n",
    "        ]\n",
    "        \n",
    "        for pattern in infobox_patterns:\n",
    "            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                # Extract family information\n",
    "                family_match = re.search(r'fam\\d+\\s*=\\s*\\[\\[(.*?)\\]\\]', match)\n",
    "                if family_match:\n",
    "                    relationships['family'].append(family_match.group(1))\n",
    "                \n",
    "                # Extract ancestor information\n",
    "                ancestor_match = re.search(r'ancestor\\d*\\s*=\\s*\\[\\[(.*?)\\]\\]', match)\n",
    "                if ancestor_match:\n",
    "                    relationships['ancestors'].append(ancestor_match.group(1))\n",
    "                \n",
    "                # Extract dialect information\n",
    "                dialect_matches = re.findall(r'dia\\d+\\s*=\\s*\\[\\[(.*?)\\]\\]', match)\n",
    "                relationships['dialects'].extend(dialect_matches)\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def get_wikidata_direct_api(self, entity_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get language data directly from Wikidata API.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'action': 'wbgetentities',\n",
    "                'ids': entity_id,\n",
    "                'format': 'json',\n",
    "                'props': 'claims|labels'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikidata_api, params=params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if entity_id not in data.get('entities', {}):\n",
    "                return {}\n",
    "            \n",
    "            entity = data['entities'][entity_id]\n",
    "            claims = entity.get('claims', {})\n",
    "            \n",
    "            # Extract relevant claims\n",
    "            relationships = {}\n",
    "            for prop_name, prop_id in self.language_properties.items():\n",
    "                if prop_id in claims:\n",
    "                    relationships[prop_name] = []\n",
    "                    for claim in claims[prop_id]:\n",
    "                        if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
    "                            value = claim['mainsnak']['datavalue']['value']\n",
    "                            if isinstance(value, dict) and 'id' in value:\n",
    "                                relationships[prop_name].append(value['id'])\n",
    "                            elif isinstance(value, str):\n",
    "                                relationships[prop_name].append(value)\n",
    "            \n",
    "            return relationships\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with Wikidata direct API: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def extract_all_relationships(self, language_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Main method to extract all language relationships using all available methods.\n",
    "        \"\"\"\n",
    "        print(f\"Extracting relationships for: {language_name}\")\n",
    "        \n",
    "        # Step 1: Get Wikidata entity ID\n",
    "        entity_id = self.get_wikidata_entity_by_name(language_name)\n",
    "        if not entity_id:\n",
    "            print(f\"Could not find Wikidata entity for {language_name}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"Found Wikidata entity: {entity_id}\")\n",
    "        \n",
    "        # Step 2: Extract relationships using different methods\n",
    "        results = {\n",
    "            'entity_id': entity_id,\n",
    "            'language_name': language_name,\n",
    "            'sparql_data': {},\n",
    "            'wikipedia_infobox': {},\n",
    "            'wikidata_api': {},\n",
    "            'combined_relationships': {\n",
    "                'genetic_descent': {'parents': [], 'children': [], 'ancestors': [], 'descendants': []},\n",
    "                'dialects': {'dialect_of': [], 'has_dialects': []},\n",
    "                'siblings': [],\n",
    "                'language_family': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # SPARQL method\n",
    "        print(\"Extracting via SPARQL...\")\n",
    "        results['sparql_data'] = self.get_language_relationships_sparql(entity_id)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        # Wikipedia infobox method\n",
    "        print(\"Extracting from Wikipedia infobox...\")\n",
    "        results['wikipedia_infobox'] = self.get_wikipedia_infobox_data(language_name)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        # Wikidata API method\n",
    "        print(\"Extracting via Wikidata API...\")\n",
    "        results['wikidata_api'] = self.get_wikidata_direct_api(entity_id)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        # Step 3: Combine and deduplicate results\n",
    "        results['combined_relationships'] = self._combine_relationships(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _combine_relationships(self, results: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Combine relationships from all sources and remove duplicates.\n",
    "        \"\"\"\n",
    "        combined = {\n",
    "            'genetic_descent': {'parents': [], 'children': [], 'ancestors': [], 'descendants': []},\n",
    "            'dialects': {'dialect_of': [], 'has_dialects': []},\n",
    "            'siblings': [],\n",
    "            'language_family': []\n",
    "        }\n",
    "        \n",
    "        # Add SPARQL results\n",
    "        sparql_data = results.get('sparql_data', {})\n",
    "        if sparql_data:\n",
    "            for category in combined:\n",
    "                if category in sparql_data:\n",
    "                    if isinstance(sparql_data[category], dict):\n",
    "                        for subcategory in combined[category]:\n",
    "                            if subcategory in sparql_data[category]:\n",
    "                                combined[category][subcategory].extend(sparql_data[category][subcategory])\n",
    "                    elif isinstance(sparql_data[category], list):\n",
    "                        combined[category].extend(sparql_data[category])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        for category in combined:\n",
    "            if isinstance(combined[category], dict):\n",
    "                for subcategory in combined[category]:\n",
    "                    combined[category][subcategory] = self._deduplicate_list(combined[category][subcategory])\n",
    "            elif isinstance(combined[category], list):\n",
    "                combined[category] = self._deduplicate_list(combined[category])\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def _deduplicate_list(self, items: List) -> List:\n",
    "        \"\"\"\n",
    "        Remove duplicates from a list of dictionaries or strings.\n",
    "        \"\"\"\n",
    "        if not items:\n",
    "            return []\n",
    "        \n",
    "        if isinstance(items[0], dict):\n",
    "            seen = set()\n",
    "            unique_items = []\n",
    "            for item in items:\n",
    "                identifier = item.get('id', str(item))\n",
    "                if identifier not in seen:\n",
    "                    seen.add(identifier)\n",
    "                    unique_items.append(item)\n",
    "            return unique_items\n",
    "        else:\n",
    "            return list(set(items))\n",
    "    \n",
    "    def save_results_csv(self, results: Dict, filename: str):\n",
    "        \"\"\"\n",
    "        Save results to CSV format for easy analysis.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        # Flatten the relationship data\n",
    "        combined = results.get('combined_relationships', {})\n",
    "        \n",
    "        # Genetic descent relationships\n",
    "        for parent in combined['genetic_descent']['parents']:\n",
    "            rows.append({\n",
    "                'source_language': results['language_name'],\n",
    "                'source_id': results['entity_id'],\n",
    "                'relationship_type': 'parent',\n",
    "                'target_language': parent.get('label', ''),\n",
    "                'target_id': parent.get('id', ''),\n",
    "                'relationship_category': 'genetic_descent'\n",
    "            })\n",
    "        \n",
    "        for child in combined['genetic_descent']['children']:\n",
    "            rows.append({\n",
    "                'source_language': results['language_name'],\n",
    "                'source_id': results['entity_id'],\n",
    "                'relationship_type': 'child',\n",
    "                'target_language': child.get('label', ''),\n",
    "                'target_id': child.get('id', ''),\n",
    "                'relationship_category': 'genetic_descent'\n",
    "            })\n",
    "        \n",
    "        # Dialect relationships\n",
    "        for dialect_parent in combined['dialects']['dialect_of']:\n",
    "            rows.append({\n",
    "                'source_language': results['language_name'],\n",
    "                'source_id': results['entity_id'],\n",
    "                'relationship_type': 'dialect_of',\n",
    "                'target_language': dialect_parent.get('label', ''),\n",
    "                'target_id': dialect_parent.get('id', ''),\n",
    "                'relationship_category': 'dialect'\n",
    "            })\n",
    "        \n",
    "        # Sibling relationships\n",
    "        for sibling in combined['siblings']:\n",
    "            rows.append({\n",
    "                'source_language': results['language_name'],\n",
    "                'source_id': results['entity_id'],\n",
    "                'relationship_type': 'sibling',\n",
    "                'target_language': sibling.get('label', ''),\n",
    "                'target_id': sibling.get('id', ''),\n",
    "                'relationship_category': 'sibling'\n",
    "            })\n",
    "        \n",
    "        # Language family\n",
    "        for family in combined['language_family']:\n",
    "            rows.append({\n",
    "                'source_language': results['language_name'],\n",
    "                'source_id': results['entity_id'],\n",
    "                'relationship_type': 'member_of_family',\n",
    "                'target_language': family.get('label', ''),\n",
    "                'target_id': family.get('id', ''),\n",
    "                'relationship_category': 'language_family'\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Example usage and demonstration\n",
    "print(\"Language Relationship Extraction Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize the extractor\n",
    "extractor = LanguageRelationshipExtractor()\n",
    "\n",
    "# Show the key properties and classes we'll be using\n",
    "print(\"\\nKey Wikidata Properties for Language Relationships:\")\n",
    "for name, prop_id in extractor.language_properties.items():\n",
    "    print(f\"  {name}: {prop_id}\")\n",
    "\n",
    "print(\"\\nKey Language Classes in Wikidata:\")\n",
    "for name, class_id in extractor.language_classes.items():\n",
    "    print(f\"  {name}: {class_id}\")\n",
    "rels=extractor.extract_all_relationships(\"English\")\n",
    "print(rels)\n",
    "print(\"\\nPipeline created successfully!\")\n",
    "print(\"Ready to extract language relationships.\")\n",
    "\n",
    "# Flatten `rels` into {language_Name1, relationship, Language_Name2} triples\n",
    "source_lang = rels.get('language_name', 'Unknown')\n",
    "cr = rels.get('combined_relationships', {}) or {}\n",
    "\n",
    "triples = []\n",
    "\n",
    "\n",
    "def _label_of(item):\n",
    "    if isinstance(item, dict):\n",
    "        return item.get('label') or item.get('id') or str(item)\n",
    "    return str(item)\n",
    "\n",
    "# Genetic descent: parents, children, ancestors, descendants\n",
    "for rel_name in ['parents', 'children', 'ancestors', 'descendants']:\n",
    "    for tgt in (cr.get('genetic_descent', {}) or {}).get(rel_name, []) or []:\n",
    "        triples.append({\n",
    "            'language_Name1': source_lang,\n",
    "            'relationship': rel_name[:-1] if rel_name.endswith('s') else rel_name,\n",
    "            'Language_Name2': _label_of(tgt)\n",
    "        })\n",
    "\n",
    "# Dialects\n",
    "for rel_name in ['dialect_of', 'has_dialects']:\n",
    "    for tgt in (cr.get('dialects', {}) or {}).get(rel_name, []) or []:\n",
    "        name = 'has_dialect' if rel_name == 'has_dialects' else 'dialect_of'\n",
    "        triples.append({\n",
    "            'language_Name1': source_lang,\n",
    "            'relationship': name,\n",
    "            'Language_Name2': _label_of(tgt)\n",
    "        })\n",
    "\n",
    "# Siblings\n",
    "for tgt in cr.get('siblings', []) or []:\n",
    "    triples.append({\n",
    "        'language_Name1': source_lang,\n",
    "        'relationship': 'sibling',\n",
    "        'Language_Name2': _label_of(tgt)\n",
    "    })\n",
    "\n",
    "# Language family\n",
    "for tgt in cr.get('language_family', []) or []:\n",
    "    triples.append({\n",
    "        'language_Name1': source_lang,\n",
    "        'relationship': 'language_family',\n",
    "        'Language_Name2': _label_of(tgt)\n",
    "    })\n",
    "\n",
    "# Deduplicate\n",
    "seen = set()\n",
    "relation_triples = []\n",
    "for t in triples:\n",
    "    key = (t['language_Name1'], t['relationship'], t['Language_Name2'])\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        relation_triples.append(t)\n",
    "\n",
    "# Print in requested compact form\n",
    "for t in relation_triples:\n",
    "    print(f\"{{{t['language_Name1']}, {t['relationship']}, {t['Language_Name2']}}}\")\n",
    "\n",
    "relation_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37825b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
