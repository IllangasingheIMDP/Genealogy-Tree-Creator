{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fc137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries: 214\n",
      "[{'language1': 'English language in Ukraine', 'relationship': 'Child of', 'language2': 'languages of Ukraine'}, {'language1': 'received pronunciation', 'relationship': 'Child of', 'language2': 'Standard English'}, {'language1': 'Gustavia English', 'relationship': 'Child of', 'language2': 'Caribbean English'}, {'language1': 'Palauan English', 'relationship': 'Child of', 'language2': 'English'}, {'language1': 'Jewish English varieties', 'relationship': 'Child of', 'language2': 'Jewish languages'}, {'language1': 'Quebec English', 'relationship': 'Child of', 'language2': 'Canadian English'}, {'language1': 'Zambian English', 'relationship': 'Child of', 'language2': 'English'}, {'language1': 'Belizean English', 'relationship': 'Child of', 'language2': 'Caribbean English'}, {'language1': 'English in the Netherlands', 'relationship': 'Child of', 'language2': 'English'}, {'language1': 'Pacific Northwest English', 'relationship': 'Child of', 'language2': 'North American English'}]\n"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "\n",
    "WIKIDATA_SPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "HEADERS = {\"User-Agent\": \"LanguageFamilyTreeBot/1.0 (https://example.com)\", \"Accept\": \"application/json\"}\n",
    "# API URLs\n",
    "WIKIPEDIA_API: str = \"https://en.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API: str = \"https://www.wikidata.org/wiki/Special:EntityData/{}.json\"\n",
    "SPARQL_API: str = \"https://query.wikidata.org/sparql\"\n",
    "WIKIDATA_QUERY_API: str = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "MAX_QIDS_PER_CALL = 50  # Wikidata wbgetentities practical limit\n",
    "MAX_RETRIES = 4\n",
    "BACKOFF_BASE = 0.8\n",
    "MAX_NODES = 1500  # safety cap to avoid runaway expansion\n",
    "\n",
    "\n",
    "def safe_get_json(url: str, *, params: dict, headers: dict | None = None):\n",
    "    \"\"\"GET a JSON response with retry & backoff; return None on hard failure.\"\"\"\n",
    "    merged_headers = {**HEADERS, **(headers or {})}\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=merged_headers, timeout=20)\n",
    "            status = resp.status_code\n",
    "            if status == 429:  # rate limited\n",
    "                wait = BACKOFF_BASE * attempt * 2\n",
    "                print(f\"Rate limited (429). Sleeping {wait:.2f}s ...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            if status >= 500:\n",
    "                wait = BACKOFF_BASE * attempt\n",
    "                print(f\"Server error {status}. Retry {attempt}/{MAX_RETRIES} after {wait:.2f}s\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            if status != 200:\n",
    "                print(f\"Non-200 status {status} for {url} params={params}\")\n",
    "                return None\n",
    "            text = resp.text.strip()\n",
    "            if not text:\n",
    "                wait = BACKOFF_BASE * attempt\n",
    "                print(f\"Empty body. Retry {attempt}/{MAX_RETRIES} after {wait:.2f}s\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            return resp.json()\n",
    "        except ValueError as ve:  # JSON decode\n",
    "            wait = BACKOFF_BASE * attempt\n",
    "            print(f\"JSON decode error attempt {attempt}: {ve}. Backing off {wait:.2f}s\")\n",
    "            time.sleep(wait)\n",
    "        except requests.RequestException as re:\n",
    "            wait = BACKOFF_BASE * attempt\n",
    "            print(f\"Request error attempt {attempt}: {re}. Backing off {wait:.2f}s\")\n",
    "            time.sleep(wait)\n",
    "    return None\n",
    "\n",
    "\n",
    "def chunked(iterable, size):\n",
    "    it = list(iterable)\n",
    "    for i in range(0, len(it), size):\n",
    "        yield it[i:i + size]\n",
    "\n",
    "\n",
    "def get_language_labels(qids):\n",
    "    \"\"\"Batch-fetch labels for a set of Q-ids (returns dict). Robust with retries & chunking.\"\"\"\n",
    "    qids = list({q for q in qids if q})\n",
    "    if not qids:\n",
    "        return {}\n",
    "    results = {}\n",
    "    for group in chunked(qids, MAX_QIDS_PER_CALL):\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"ids\": \"|\".join(group),\n",
    "            \"props\": \"labels\",\n",
    "            \"languages\": \"en\",\n",
    "            \"format\": \"json\",\n",
    "            \"origin\": \"*\",\n",
    "        }\n",
    "        data = safe_get_json(WIKIDATA_QUERY_API, params=params)\n",
    "        if not data:\n",
    "            for q in group:  # leave unresolved\n",
    "                results.setdefault(q, q)\n",
    "            continue\n",
    "        entities = data.get(\"entities\", {})\n",
    "        for qid, ent in entities.items():\n",
    "            label = ent.get(\"labels\", {}).get(\"en\", {}).get(\"value\")\n",
    "            if label:\n",
    "                results[qid] = label\n",
    "            else:\n",
    "                results.setdefault(qid, qid)\n",
    "        time.sleep(0.1)  # politeness\n",
    "    return results\n",
    "\n",
    "# Simple caches\n",
    "LABEL_CACHE: dict[str, str] = {}\n",
    "VALID_QIDS: set[str] = set()\n",
    "INVALID_QIDS: set[str] = set()\n",
    "\n",
    "\n",
    "def is_valid_language(qid):\n",
    "    \"\"\"Check if a QID represents a valid language, dialect, or language family.\"\"\"\n",
    "    if not qid:\n",
    "        return False\n",
    "    query = f\"\"\"\n",
    "    SELECT ?class WHERE {{\n",
    "      wd:{qid} (wdt:P31/wdt:P279*) ?class.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    classes = [r['class']['value'].split('/')[-1] for r in results]\n",
    "    valid_classes = [\n",
    "        'Q34770',    # language\n",
    "        'Q33742',    # natural language\n",
    "        'Q20162172', # human language\n",
    "        'Q33384',    # dialect\n",
    "        'Q25209536', # variety of language\n",
    "        'Q1288568',  # modern language\n",
    "        'Q25295',    # language family\n",
    "        'Q1072694',  # constructed language\n",
    "        'Q17376908', # language isolate\n",
    "        'Q11755682', # proto-language\n",
    "        \"Q45762\"\n",
    "    ]\n",
    "    return any(c in valid_classes for c in classes)\n",
    "\n",
    "\n",
    "def is_valid_language_cached(qid: str) -> bool:\n",
    "    if qid in VALID_QIDS:\n",
    "        return True\n",
    "    if qid in INVALID_QIDS:\n",
    "        return False\n",
    "    ok = is_valid_language(qid)\n",
    "    (VALID_QIDS if ok else INVALID_QIDS).add(qid)\n",
    "    return ok\n",
    "\n",
    "\n",
    "def get_label(qid: str) -> str:\n",
    "    \"\"\"Return English label for a QID (falls back to QID).\"\"\"\n",
    "    if not qid:\n",
    "        return qid\n",
    "    if qid in LABEL_CACHE:\n",
    "        return LABEL_CACHE[qid]\n",
    "    labels = get_language_labels([qid])\n",
    "    label = labels.get(qid, qid)\n",
    "    LABEL_CACHE[qid] = label\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_wikidata_entity_id(language_name):\n",
    "    \"\"\"Return the Wikidata Q-identifier for a language name.\"\"\"\n",
    "    try:\n",
    "        # First try direct page lookup\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"titles\": f\"{language_name} language\",\n",
    "            \"prop\": \"pageprops\",\n",
    "            \"ppprop\": \"wikibase_item\",\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        data = safe_get_json(WIKIPEDIA_API, params=params) or {}\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        for page in pages.values():\n",
    "            if \"pageprops\" in page and \"wikibase_item\" in page[\"pageprops\"]:\n",
    "                return page[\"pageprops\"][\"wikibase_item\"]\n",
    "        # Search fallback\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": f\"{language_name} language\",\n",
    "            \"srlimit\": 1,\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        data = safe_get_json(WIKIPEDIA_API, params=params) or {}\n",
    "        search = data.get(\"query\", {}).get(\"search\", [])\n",
    "        if search:\n",
    "            page_title = search[0][\"title\"]\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"titles\": page_title,\n",
    "                \"prop\": \"pageprops\",\n",
    "                \"ppprop\": \"wikibase_item\",\n",
    "                \"format\": \"json\",\n",
    "            }\n",
    "            data = safe_get_json(WIKIPEDIA_API, params=params) or {}\n",
    "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "            for page in pages.values():\n",
    "                if \"pageprops\" in page and \"wikibase_item\" in page[\"pageprops\"]:\n",
    "                    return page[\"pageprops\"][\"wikibase_item\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting QID for {language_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_parents(entity_id):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?parent ?parentLabel WHERE {{\n",
    "      wd:{entity_id} wdt:P279 ?parent.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    return [(r['parent']['value'].split('/')[-1], r['parentLabel']['value']) for r in results]\n",
    "\n",
    "\n",
    "def get_children_by_p527(entity_id):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?child ?childLabel WHERE {{\n",
    "      wd:{entity_id} wdt:P527 ?child.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    return [(r['child']['value'].split('/')[-1], r['childLabel']['value']) for r in results]\n",
    "\n",
    "\n",
    "def get_children(entity_id):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?child ?childLabel WHERE {{\n",
    "      ?child wdt:P279 wd:{entity_id}.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    return [(r['child']['value'].split('/')[-1], r['childLabel']['value']) for r in results]\n",
    "\n",
    "\n",
    "def build_language_family_tree(entity_id, depth, current_depth=1, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if current_depth > depth or entity_id in visited:\n",
    "        return []\n",
    "    if len(visited) >= MAX_NODES:\n",
    "        print(\"Max node cap reached; stopping expansion.\")\n",
    "        return []\n",
    "\n",
    "    visited.add(entity_id)\n",
    "    relations: list[tuple[str, str, str]] = []\n",
    "    current_label = get_label(entity_id)\n",
    "\n",
    "    # Parents (superclasses). Filter invalid before using.\n",
    "    parents = get_parents(entity_id)\n",
    "    if entity_id == \"Q1680\":  # debug for English\n",
    "        print(\"Parents of English:\", parents)\n",
    "    for parent_id, parent_label in parents:\n",
    "        if is_valid_language_cached(parent_id):\n",
    "            relations.append((current_label, \"Child of\", parent_label))\n",
    "            relations.extend(build_language_family_tree(parent_id, depth, current_depth + 1, visited))\n",
    "\n",
    "    # Children by P527 (parts/members)\n",
    "    for child_id, child_label in get_children_by_p527(entity_id):\n",
    "        if child_id != entity_id and is_valid_language_cached(child_id):\n",
    "            relations.append((child_label, \"Child of\", current_label))\n",
    "            relations.extend(build_language_family_tree(child_id, depth, current_depth + 1, visited))\n",
    "\n",
    "    # Children by reverse P279 (subclasses)\n",
    "    for child_id, child_label in get_children(entity_id):\n",
    "        if child_id != entity_id and child_id not in visited and is_valid_language_cached(child_id):\n",
    "            relations.append((child_label, \"Child of\", current_label))\n",
    "            relations.extend(build_language_family_tree(child_id, depth, current_depth + 1, visited))\n",
    "\n",
    "    return relations\n",
    "\n",
    "\n",
    "def get_language_family(language_name, depth):\n",
    "    entity_id = get_wikidata_entity_id(language_name)\n",
    "    if not entity_id:\n",
    "        raise ValueError(f\"Language '{language_name}' not found in Wikidata.\")\n",
    "    # Root language is forced valid to ensure at least a starting point\n",
    "    VALID_QIDS.add(entity_id)\n",
    "    relations = build_language_family_tree(entity_id, depth)\n",
    "    unique_relations = list({(r[0], r[1], r[2]) for r in relations})\n",
    "    formatted_relations = [{\"language1\": rel[0], \"relationship\": rel[1], \"language2\": rel[2]} for rel in unique_relations]\n",
    "    return formatted_relations\n",
    "\n",
    "# Example usage\n",
    "family_tree = get_language_family(\"English\", 2)\n",
    "print(f\"Entries: {len(family_tree)}\")\n",
    "print(family_tree[:10])  # preview first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b64300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Reserved cell) – helper / scratch space.\n",
    "# Visualization cells will follow below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91a369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 193, Edges: 214\n",
      "Depth levels computed (sample): [('languages of Ukraine', 0), ('Jewish languages', 0), ('languages of Sweden', 0), ('languages of the Falkland Islands', 0), ('languages of Europe', 0), ('languages of Switzerland', 0), ('languages of Puerto Rico', 0), ('languages of Lebanon', 0), ('Anglo-Frisian', 0), ('languages of Spain', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Build a NetworkX graph from the family_tree list of dicts (language1, relationship, language2)\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    raise SystemExit(\"Please install networkx: pip install networkx\")\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "if 'family_tree' not in globals():\n",
    "    raise RuntimeError(\"family_tree not defined – run the retrieval cell first.\")\n",
    "\n",
    "def build_graph(relations):\n",
    "    G = nx.DiGraph()\n",
    "    for rel in relations:\n",
    "        l1 = rel[\"language1\"]\n",
    "        l2 = rel[\"language2\"]\n",
    "        label = rel[\"relationship\"]\n",
    "        # Edge direction: parent -> child (reverse of 'Child of')\n",
    "        if label == \"Child of\":\n",
    "            parent, child = l2, l1\n",
    "        else:\n",
    "            parent, child = l1, l2\n",
    "        G.add_node(parent)\n",
    "        G.add_node(child)\n",
    "        G.add_edge(parent, child, relationship=label)\n",
    "    return G\n",
    "\n",
    "family_graph = build_graph(family_tree)\n",
    "print(f\"Nodes: {family_graph.number_of_nodes()}, Edges: {family_graph.number_of_edges()}\")\n",
    "\n",
    "# Derive depths (heuristic BFS from nodes with no predecessors)\n",
    "roots = [n for n in family_graph.nodes() if family_graph.in_degree(n) == 0]\n",
    "node_depth = {n: 0 for n in roots}\n",
    "for r in roots:\n",
    "    q = deque([(r, 0)])\n",
    "    while q:\n",
    "        node, d = q.popleft()\n",
    "        for child in family_graph.successors(node):\n",
    "            if child not in node_depth or d + 1 < node_depth[child]:\n",
    "                node_depth[child] = d + 1\n",
    "                q.append((child, d + 1))\n",
    "print(\"Depth levels computed (sample):\", list(node_depth.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b55aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m net = Network(height=\u001b[33m'\u001b[39m\u001b[33m750px\u001b[39m\u001b[33m'\u001b[39m, width=\u001b[33m'\u001b[39m\u001b[33m100\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m, directed=\u001b[38;5;28;01mTrue\u001b[39;00m, notebook=\u001b[38;5;28;01mTrue\u001b[39;00m, bgcolor=\u001b[33m'\u001b[39m\u001b[33m#ffffff\u001b[39m\u001b[33m'\u001b[39m, font_color=\u001b[33m'\u001b[39m\u001b[33m#222222\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m net.toggle_physics(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_options\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43mvar options = \u001b[39;49m\u001b[33;43m{\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m  physics: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m stabilization: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m iterations: 150 }, barnesHut: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m gravitationalConstant: -4000 } },\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m  edges: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m arrows: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m to: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m enabled: true } }, smooth: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m type: \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdynamic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m } }\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m}\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m family_graph.nodes():\n\u001b[32m     25\u001b[39m     depth = \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mnode_depth\u001b[39m\u001b[33m'\u001b[39m, {}).get(node, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyvis\\network.py:1006\u001b[39m, in \u001b[36mNetwork.set_options\u001b[39m\u001b[34m(self, options)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_options\u001b[39m(\u001b[38;5;28mself\u001b[39m, options):\n\u001b[32m    997\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[33;03m    Overrides the default options object passed to the VisJS framework.\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[33;03m    Delegates to the :meth:`options.Options.set` routine.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m \u001b[33;03m    :type options: str\u001b[39;00m\n\u001b[32m   1005\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28mself\u001b[39m.options = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyvis\\options.py:224\u001b[39m, in \u001b[36mOptions.set\u001b[39m\u001b[34m(self, new_options)\u001b[39m\n\u001b[32m    222\u001b[39m first_bracket = options.find(\u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    223\u001b[39m options = options[first_bracket:]\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m options = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m options\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:353\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \n\u001b[32m    351\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "# Interactive visualization with PyVis (HTML)\n",
    "try:\n",
    "    from pyvis.network import Network\n",
    "except ImportError:\n",
    "    raise SystemExit(\"Please install pyvis: pip install pyvis\")\n",
    "\n",
    "if 'family_graph' not in globals():\n",
    "    raise RuntimeError(\"family_graph not defined – run the graph build cell first.\")\n",
    "\n",
    "palette = [\n",
    "    '#1f77b4', '#2ca02c', '#ff7f0e', '#9467bd', '#8c564b',\n",
    "    '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'\n",
    "]\n",
    "\n",
    "net = Network(height='750px', width='100%', directed=True, notebook=True, bgcolor='#ffffff', font_color='#222222')\n",
    "net.toggle_physics(True)\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  physics: { stabilization: { iterations: 150 }, barnesHut: { gravitationalConstant: -4000 } },\n",
    "  edges: { arrows: { to: { enabled: true } }, smooth: { type: 'dynamic' } }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "for node in family_graph.nodes():\n",
    "    depth = globals().get('node_depth', {}).get(node, 0)\n",
    "    color = palette[depth % len(palette)]\n",
    "    net.add_node(node, label=node, color=color, level=depth)\n",
    "\n",
    "for u, v, data in family_graph.edges(data=True):\n",
    "    rel = data.get('relationship', '')\n",
    "    net.add_edge(u, v, title=rel)\n",
    "\n",
    "net.show('language_family_tree.html')\n",
    "print(\"Generated language_family_tree.html (open in sidebar or a browser).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706fc14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "languages of Ukraine [d=0]\n",
      "└─ English language in Ukraine [d=1]\n",
      "Jewish languages [d=0]\n",
      "└─ Jewish English varieties [d=1]\n",
      "languages of Sweden [d=0]\n",
      "└─ English language in Sweden [d=1]\n",
      "languages of the Falkland Islands [d=0]\n",
      "└─ Falkland Islands English [d=1]\n",
      "languages of Europe [d=0]\n",
      "└─ English language in Europe [d=1]\n",
      "languages of Switzerland [d=0]\n",
      "└─ English language in Switzerland [d=1]\n",
      "languages of Puerto Rico [d=0]\n",
      "└─ English language in Puerto Rico [d=1]\n",
      "languages of Lebanon [d=0]\n",
      "└─ English language in Lebanon [d=1]\n",
      "Anglo-Frisian [d=0]\n",
      "└─ Anglic [d=1]\n",
      "└─ ├─ Yola [d=2]\n",
      "└─ ├─ Middle English [d=2]\n",
      "└─ ├─ ├─ Kentish Dialect [d=3]\n",
      "└─ ├─ ├─ Southern Dialect [d=3]\n",
      "└─ ├─ ├─ Central Middle English [d=3]\n",
      "└─ ├─ ├─ Southern Middle English [d=3]\n",
      "└─ ├─ ├─ East Midland Dialect [d=3]\n",
      "└─ ├─ ├─ Northern Middle English [d=3]\n",
      "└─ ├─ ├─ West Midland Dialect [d=3]\n",
      "└─ ├─ ├─ Late Middle English [d=3]\n",
      "└─ ├─ ├─ Northern Dialect [d=3]\n",
      "└─ ├─ ├─ Midland Middle English [d=3]\n",
      "└─ ├─ ├─ Q1877420 [d=3]\n",
      "└─ ├─ └─ Early Middle English [d=3]\n",
      "└─ ├─ Scots [d=2]\n",
      "└─ ├─ English [d=2]\n",
      "└─ ├─ ├─ Palauan English [d=3]\n",
      "└─ ├─ ├─ Zambian English [d=3]\n",
      "└─ ├─ ├─ English in the Netherlands [d=1]\n",
      "└─ ├─ ├─ Basic English [d=3]\n",
      "└─ ├─ ├─ English as a second or foreign language [d=3]\n",
      "└─ ├─ ├─ African American Vernacular English [d=3]\n",
      "└─ ├─ ├─ ├─ Philadelphia English [d=4]\n",
      "└─ ├─ ├─ ├─ Washington, D.C., African American Vernacular English [d=4]\n",
      "└─ ├─ ├─ ├─ New York dialect [d=4]\n",
      "└─ ├─ ├─ └─ Baltimore dialect [d=4]\n",
      "└─ ├─ ├─ Ugandan English [d=3]\n",
      "└─ ├─ ├─ British English [d=3]\n",
      "└─ ├─ ├─ ├─ English language in England [d=4]\n",
      "└─ ├─ ├─ ├─ Australian English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ South Australian English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Neo-Nyungar [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Western Australian English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Australian Aboriginal English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Standard Australian English [d=4]\n",
      "└─ ├─ ├─ ├─ └─ Strine [d=4]\n",
      "└─ ├─ ├─ ├─ Welsh English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ Port Talbot English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Cardiff English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Abercraf English [d=4]\n",
      "└─ ├─ ├─ ├─ └─ Gower dialect [d=4]\n",
      "└─ ├─ ├─ ├─ Multicultural British English [d=4]\n",
      "└─ ├─ ├─ ├─ received pronunciation [d=3]\n",
      "└─ ├─ ├─ └─ Surrey dialect [d=4]\n",
      "└─ ├─ ├─ International English [d=3]\n",
      "└─ ├─ ├─ └─ Antarctic English [d=4]\n",
      "└─ ├─ ├─ English language in Lebanon [d=1]\n",
      "└─ ├─ ├─ Ozark English [d=3]\n",
      "└─ ├─ ├─ Scottish English [d=3]\n",
      "└─ ├─ ├─ └─ Highland English [d=4]\n",
      "└─ ├─ ├─ Solomon Islands English [d=3]\n",
      "└─ ├─ ├─ Hebronics [d=3]\n",
      "└─ ├─ ├─ Bahamian English [d=3]\n",
      "└─ ├─ ├─ Welsh English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Bay Islands English [d=3]\n",
      "└─ ├─ ├─ Gibraltarian English [d=3]\n",
      "└─ ├─ ├─ Tristan da Cunha English [d=3]\n",
      "└─ ├─ ├─ Trinidadian and Tobagonian English [d=3]\n",
      "└─ ├─ ├─ Tinglish [d=3]\n",
      "└─ ├─ ├─ English language in Algeria [d=1]\n",
      "└─ ├─ ├─ Ghanaian English [d=3]\n",
      "└─ ├─ ├─ South African English [d=3]\n",
      "└─ ├─ ├─ └─ Cape Flats English [d=4]\n",
      "└─ ├─ ├─ Bajan English [d=3]\n",
      "└─ ├─ ├─ Pirate English [d=3]\n",
      "└─ ├─ ├─ Nigerian English [d=3]\n",
      "└─ ├─ ├─ Falkland Islands English [d=1]\n",
      "└─ ├─ ├─ Indian English [d=3]\n",
      "└─ ├─ ├─ └─ Myanmar English [d=4]\n",
      "└─ ├─ ├─ South Atlantic English [d=3]\n",
      "└─ ├─ ├─ Ulster English [d=3]\n",
      "└─ ├─ ├─ English language in Ukraine [d=1]\n",
      "└─ ├─ ├─ English language in Spain [d=1]\n",
      "└─ ├─ ├─ English language in Puerto Rico [d=1]\n",
      "└─ ├─ ├─ Pig Latin [d=3]\n",
      "└─ ├─ ├─ English language in Switzerland [d=1]\n",
      "└─ ├─ ├─ Pennsylvania Dutch English [d=3]\n",
      "└─ ├─ ├─ Luminar English [d=3]\n",
      "└─ ├─ ├─ Ebonics [d=3]\n",
      "└─ ├─ ├─ Buddhist Hybrid English [d=3]\n",
      "└─ ├─ ├─ Solomon Islands Pidgin English [d=3]\n",
      "└─ ├─ ├─ Namlish [d=3]\n",
      "└─ ├─ ├─ English language in Europe [d=1]\n",
      "└─ ├─ ├─ African Nova Scotian English [d=3]\n",
      "└─ ├─ ├─ Gambian English [d=3]\n",
      "└─ ├─ ├─ Newfoundland English [d=3]\n",
      "└─ ├─ ├─ Cameroonian English [d=3]\n",
      "└─ ├─ ├─ North American English [d=3]\n",
      "└─ ├─ ├─ ├─ Pacific Northwest English [d=4]\n",
      "└─ ├─ ├─ ├─ Mexican English [d=4]\n",
      "└─ ├─ ├─ ├─ Bermudian English [d=4]\n",
      "└─ ├─ ├─ ├─ General American English [d=4]\n",
      "└─ ├─ ├─ ├─ Jamaican English [d=3]\n",
      "└─ ├─ ├─ ├─ American English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ Northern American English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ General American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Chicano English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Southern American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ └─ Ozark English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ └─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Philadelphia English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ North–Central American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Older Southern American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Alaskan English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Western Pennsylvania English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Appalachian English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Inland Northern American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Miami English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ California English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New York Latino English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New York dialect [d=4]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ African American Vernacular English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Boontling [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New Orleans English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ African-American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ └─ African American Vernacular English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ └─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Q20007967 [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Mid-Atlantic accent [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Yooper dialect [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Western American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New England English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ High Tider [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New Jersey English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Cajun English [d=4]\n",
      "└─ ├─ ├─ ├─ └─ Midland American English [d=4]\n",
      "└─ ├─ ├─ └─ Canadian English [d=3]\n",
      "└─ ├─ ├─ └─ ├─ Quebec English [d=3]\n",
      "└─ ├─ ├─ └─ ├─ Aboriginal English [d=4]\n",
      "└─ ├─ ├─ └─ ├─ Ottawa Valley English [d=4]\n",
      "└─ ├─ ├─ └─ ├─ Standard Canadian English [d=4]\n",
      "└─ ├─ ├─ └─ └─ Multicultural Toronto English [d=4]\n",
      "└─ ├─ ├─ Hawaiian English [d=3]\n",
      "└─ ├─ ├─ Singapore English [d=3]\n",
      "└─ ├─ ├─ South Asian English [d=3]\n",
      "└─ ├─ ├─ ├─ Maldivian English [d=4]\n",
      "└─ ├─ ├─ ├─ Bangladeshi English [d=4]\n",
      "└─ ├─ ├─ ├─ Sri Lankan English [d=4]\n",
      "└─ ├─ ├─ ├─ Indian English [d=3]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ Pakistani English [d=3]\n",
      "└─ ├─ ├─ └─ Nepali English [d=4]\n",
      "└─ ├─ ├─ Zimbabwean English [d=3]\n",
      "└─ ├─ ├─ Malawian English [d=3]\n",
      "└─ ├─ ├─ Butler English [d=3]\n",
      "└─ ├─ ├─ Quebec English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Standard English [d=3]\n",
      "└─ ├─ ├─ ├─ received pronunciation [d=3]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ Standard Australian English [d=4]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ └─ General American English [d=4]\n",
      "└─ ├─ ├─ └─   (cycle)\n",
      "└─ ├─ ├─ New Zealand English [d=3]\n",
      "└─ ├─ ├─ Kenyan English [d=3]\n",
      "└─ ├─ ├─ Northern American English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Singlish [d=3]\n",
      "└─ ├─ ├─ Jewish English varieties [d=1]\n",
      "└─ ├─ ├─ Pakistani English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Southeast Asian English [d=3]\n",
      "└─ ├─ ├─ Philippine English [d=3]\n",
      "└─ ├─ ├─ └─ Englog [d=4]\n",
      "└─ ├─ ├─ └─ └─ Philippine English [d=3]\n",
      "└─ ├─ ├─ └─ └─   (cycle)\n",
      "└─ ├─ ├─ received pronunciation [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Torres Strait English [d=3]\n",
      "└─ ├─ ├─ Iyaric [d=3]\n",
      "└─ ├─ ├─ Euro English [d=3]\n",
      "└─ ├─ ├─ └─ Dunglish [d=4]\n",
      "└─ ├─ ├─ dialects of English [d=3]\n",
      "└─ ├─ ├─ Saint Helena English [d=3]\n",
      "└─ ├─ ├─ Globish [d=3]\n",
      "└─ ├─ ├─ Simplified Technical English [d=3]\n",
      "└─ ├─ ├─ Hiberno-English [d=3]\n",
      "└─ ├─ ├─ ├─ Shelta [d=4]\n",
      "└─ ├─ ├─ ├─ Dublin English [d=4]\n",
      "└─ ├─ ├─ └─ South-West Irish English [d=4]\n",
      "└─ ├─ ├─ Brunei English [d=3]\n",
      "└─ ├─ ├─ Middle English [d=2]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Australian English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Belizean English [d=3]\n",
      "└─ ├─ ├─ Hong Kong English [d=3]\n",
      "└─ ├─ ├─ Denglisch [d=1]\n",
      "└─ ├─ ├─ American English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Zulu English [d=3]\n",
      "└─ ├─ ├─ Simple English [d=3]\n",
      "└─ ├─ ├─ Maltese English [d=3]\n",
      "└─ ├─ ├─ Ghanaian Pidgin English [d=3]\n",
      "└─ ├─ ├─ Canadian English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Channel Island English [d=3]\n",
      "└─ ├─ ├─ Sierra Leonean English [d=3]\n",
      "└─ ├─ ├─ broken English [d=3]\n",
      "└─ ├─ ├─ English language in Sweden [d=1]\n",
      "└─ ├─ ├─ Cyrillic English [d=3]\n",
      "└─ ├─ ├─ Caribbean English [d=3]\n",
      "└─ ├─ ├─ ├─ Gustavia English [d=4]\n",
      "└─ ├─ ├─ ├─ Belizean English [d=3]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ Cayman Islands English [d=4]\n",
      "└─ ├─ ├─ ├─ Saban English [d=4]\n",
      "└─ ├─ ├─ ├─ Samaná English [d=4]\n",
      "└─ ├─ ├─ └─ Bequia English [d=4]\n",
      "└─ ├─ ├─ Jamaican English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Manx English [d=3]\n",
      "└─ ├─ └─ Utah Mormon English [d=3]\n",
      "└─ ├─ Modern English [d=2]\n",
      "└─ ├─ Fingalian [d=2]\n",
      "└─ ├─ Early Modern English [d=2]\n",
      "└─ └─ Old English [d=2]\n",
      "languages of Spain [d=0]\n",
      "└─ English language in Spain [d=1]\n",
      "German [d=0]\n",
      "└─ Denglisch [d=1]\n",
      "languages of the Netherlands [d=0]\n",
      "└─ English in the Netherlands [d=1]\n",
      "languages of Algeria [d=0]\n",
      "└─ English language in Algeria [d=1]\n"
     ]
    }
   ],
   "source": [
    "# ASCII tree fallback (choose roots heuristically: nodes with no incoming edges)\n",
    "from collections import defaultdict\n",
    "\n",
    "if 'family_graph' not in globals():\n",
    "    raise RuntimeError(\"family_graph not defined – run the graph build cell first.\")\n",
    "\n",
    "incoming = defaultdict(int)\n",
    "for u, v in family_graph.edges():\n",
    "    incoming[v] += 1\n",
    "\n",
    "roots = [n for n in family_graph.nodes() if incoming[n] == 0] or list(family_graph.nodes())[:1]\n",
    "\n",
    "PRINT_LIMIT = 400  # avoid huge console spam\n",
    "printed = 0\n",
    "\n",
    "def print_tree(node, prefix=\"\", visited=None):\n",
    "    global printed\n",
    "    if printed >= PRINT_LIMIT:\n",
    "        return\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    depth = globals().get('node_depth', {}).get(node, 0)\n",
    "    print(f\"{prefix}{node} [d={depth}]\")\n",
    "    printed += 1\n",
    "    if node in visited:\n",
    "        print(prefix + \"  (cycle)\")\n",
    "        return\n",
    "    visited.add(node)\n",
    "    children = list(family_graph.successors(node))\n",
    "    for i, child in enumerate(children):\n",
    "        if printed >= PRINT_LIMIT:\n",
    "            print(\"... output truncated ...\")\n",
    "            return\n",
    "        is_last = i == len(children) - 1\n",
    "        connector = \"└─ \" if is_last else \"├─ \"\n",
    "        print_tree(child, prefix + connector, visited)\n",
    "\n",
    "for r in roots:\n",
    "    if printed >= PRINT_LIMIT:\n",
    "        break\n",
    "    print_tree(r)\n",
    "if printed >= PRINT_LIMIT:\n",
    "    print(f\"Truncated after {PRINT_LIMIT} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69afe1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True extinct_language\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "class WikidataLanguageValidator:\n",
    "    def __init__(self):\n",
    "        self.sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "        self.valid_types = {\n",
    "            \"language\": \"wd:Q34770\",          # language\n",
    "            \"language_family\": \"wd:Q25295\", # language family\n",
    "            \"dialect\": \"wd:Q33384\"  ,           # dialect\n",
    "            \"extinct_language\":\"wd:Q38058796\",\n",
    "            \"dead_language\":\"wd:Q45762\"\n",
    "        }\n",
    "\n",
    "    def validate_qid(self, qid):\n",
    "        # SPARQL query to check instance of and subclasses for valid types\n",
    "        query = f\"\"\"\n",
    "        SELECT ?type ?typeLabel WHERE {{\n",
    "          VALUES ?item {{ wd:{qid} }} \n",
    "          {{\n",
    "            ?item wdt:P31 ?type.\n",
    "          }} UNION {{\n",
    "            ?item wdt:P279 ?type.\n",
    "          }}\n",
    "          VALUES ?validType {{ { ' '.join(self.valid_types.values())} }}\n",
    "          FILTER(?type IN (?validType))\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sparql.setQuery(query)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "        results = self.sparql.query().convert()\n",
    "\n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            type_uri = results[\"results\"][\"bindings\"][0][\"type\"][\"value\"]\n",
    "            type_label = results[\"results\"][\"bindings\"][0][\"typeLabel\"][\"value\"]\n",
    "\n",
    "            for key, val in self.valid_types.items():\n",
    "                if type_uri.endswith(val.split(\":\")[1]):\n",
    "                    return True, key\n",
    "\n",
    "        return False, None\n",
    "\n",
    "# Example usage:\n",
    "validator = WikidataLanguageValidator()\n",
    "valid, classification = validator.validate_qid(\"Q56395\")  # e.g., \"English language\" QID = Q1860\n",
    "print(valid, classification)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
