{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fc137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries: 214\n",
      "[{'language1': 'English language in Ukraine', 'relationship': 'Child of', 'language2': 'languages of Ukraine'}, {'language1': 'received pronunciation', 'relationship': 'Child of', 'language2': 'Standard English'}, {'language1': 'Gustavia English', 'relationship': 'Child of', 'language2': 'Caribbean English'}, {'language1': 'Palauan English', 'relationship': 'Child of', 'language2': 'English'}, {'language1': 'Jewish English varieties', 'relationship': 'Child of', 'language2': 'Jewish languages'}, {'language1': 'Quebec English', 'relationship': 'Child of', 'language2': 'Canadian English'}, {'language1': 'Zambian English', 'relationship': 'Child of', 'language2': 'English'}, {'language1': 'Belizean English', 'relationship': 'Child of', 'language2': 'Caribbean English'}, {'language1': 'English in the Netherlands', 'relationship': 'Child of', 'language2': 'English'}, {'language1': 'Pacific Northwest English', 'relationship': 'Child of', 'language2': 'North American English'}]\n"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "\n",
    "WIKIDATA_SPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "HEADERS = {\"User-Agent\": \"LanguageFamilyTreeBot/1.0 (https://example.com)\", \"Accept\": \"application/json\"}\n",
    "# API URLs\n",
    "WIKIPEDIA_API: str = \"https://en.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API: str = \"https://www.wikidata.org/wiki/Special:EntityData/{}.json\"\n",
    "SPARQL_API: str = \"https://query.wikidata.org/sparql\"\n",
    "WIKIDATA_QUERY_API: str = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "MAX_QIDS_PER_CALL = 50  # Wikidata wbgetentities practical limit\n",
    "MAX_RETRIES = 4\n",
    "BACKOFF_BASE = 0.8\n",
    "MAX_NODES = 1500  # safety cap to avoid runaway expansion\n",
    "\n",
    "\n",
    "def safe_get_json(url: str, *, params: dict, headers: dict | None = None):\n",
    "    \"\"\"GET a JSON response with retry & backoff; return None on hard failure.\"\"\"\n",
    "    merged_headers = {**HEADERS, **(headers or {})}\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=merged_headers, timeout=20)\n",
    "            status = resp.status_code\n",
    "            if status == 429:  # rate limited\n",
    "                wait = BACKOFF_BASE * attempt * 2\n",
    "                print(f\"Rate limited (429). Sleeping {wait:.2f}s ...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            if status >= 500:\n",
    "                wait = BACKOFF_BASE * attempt\n",
    "                print(f\"Server error {status}. Retry {attempt}/{MAX_RETRIES} after {wait:.2f}s\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            if status != 200:\n",
    "                print(f\"Non-200 status {status} for {url} params={params}\")\n",
    "                return None\n",
    "            text = resp.text.strip()\n",
    "            if not text:\n",
    "                wait = BACKOFF_BASE * attempt\n",
    "                print(f\"Empty body. Retry {attempt}/{MAX_RETRIES} after {wait:.2f}s\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            return resp.json()\n",
    "        except ValueError as ve:  # JSON decode\n",
    "            wait = BACKOFF_BASE * attempt\n",
    "            print(f\"JSON decode error attempt {attempt}: {ve}. Backing off {wait:.2f}s\")\n",
    "            time.sleep(wait)\n",
    "        except requests.RequestException as re:\n",
    "            wait = BACKOFF_BASE * attempt\n",
    "            print(f\"Request error attempt {attempt}: {re}. Backing off {wait:.2f}s\")\n",
    "            time.sleep(wait)\n",
    "    return None\n",
    "\n",
    "\n",
    "def chunked(iterable, size):\n",
    "    it = list(iterable)\n",
    "    for i in range(0, len(it), size):\n",
    "        yield it[i:i + size]\n",
    "\n",
    "\n",
    "def get_language_labels(qids):\n",
    "    \"\"\"Batch-fetch labels for a set of Q-ids (returns dict). Robust with retries & chunking.\"\"\"\n",
    "    qids = list({q for q in qids if q})\n",
    "    if not qids:\n",
    "        return {}\n",
    "    results = {}\n",
    "    for group in chunked(qids, MAX_QIDS_PER_CALL):\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"ids\": \"|\".join(group),\n",
    "            \"props\": \"labels\",\n",
    "            \"languages\": \"en\",\n",
    "            \"format\": \"json\",\n",
    "            \"origin\": \"*\",\n",
    "        }\n",
    "        data = safe_get_json(WIKIDATA_QUERY_API, params=params)\n",
    "        if not data:\n",
    "            for q in group:  # leave unresolved\n",
    "                results.setdefault(q, q)\n",
    "            continue\n",
    "        entities = data.get(\"entities\", {})\n",
    "        for qid, ent in entities.items():\n",
    "            label = ent.get(\"labels\", {}).get(\"en\", {}).get(\"value\")\n",
    "            if label:\n",
    "                results[qid] = label\n",
    "            else:\n",
    "                results.setdefault(qid, qid)\n",
    "        time.sleep(0.1)  # politeness\n",
    "    return results\n",
    "\n",
    "# Simple caches\n",
    "LABEL_CACHE: dict[str, str] = {}\n",
    "VALID_QIDS: set[str] = set()\n",
    "INVALID_QIDS: set[str] = set()\n",
    "\n",
    "\n",
    "def is_valid_language(qid):\n",
    "    \"\"\"Check if a QID represents a valid language, dialect, or language family.\"\"\"\n",
    "    if not qid:\n",
    "        return False\n",
    "    query = f\"\"\"\n",
    "    SELECT ?class WHERE {{\n",
    "      wd:{qid} (wdt:P31/wdt:P279*) ?class.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    classes = [r['class']['value'].split('/')[-1] for r in results]\n",
    "    valid_classes = [\n",
    "        'Q34770',    # language\n",
    "        'Q33742',    # natural language\n",
    "        'Q20162172', # human language\n",
    "        'Q33384',    # dialect\n",
    "        'Q25209536', # variety of language\n",
    "        'Q1288568',  # modern language\n",
    "        'Q25295',    # language family\n",
    "        'Q1072694',  # constructed language\n",
    "        'Q17376908', # language isolate\n",
    "        'Q11755682', # proto-language\n",
    "        \"Q45762\"\n",
    "    ]\n",
    "    return any(c in valid_classes for c in classes)\n",
    "\n",
    "\n",
    "def is_valid_language_cached(qid: str) -> bool:\n",
    "    if qid in VALID_QIDS:\n",
    "        return True\n",
    "    if qid in INVALID_QIDS:\n",
    "        return False\n",
    "    ok = is_valid_language(qid)\n",
    "    (VALID_QIDS if ok else INVALID_QIDS).add(qid)\n",
    "    return ok\n",
    "\n",
    "\n",
    "def get_label(qid: str) -> str:\n",
    "    \"\"\"Return English label for a QID (falls back to QID).\"\"\"\n",
    "    if not qid:\n",
    "        return qid\n",
    "    if qid in LABEL_CACHE:\n",
    "        return LABEL_CACHE[qid]\n",
    "    labels = get_language_labels([qid])\n",
    "    label = labels.get(qid, qid)\n",
    "    LABEL_CACHE[qid] = label\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_wikidata_entity_id(language_name):\n",
    "    \"\"\"Return the Wikidata Q-identifier for a language name.\"\"\"\n",
    "    try:\n",
    "        # First try direct page lookup\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"titles\": f\"{language_name} language\",\n",
    "            \"prop\": \"pageprops\",\n",
    "            \"ppprop\": \"wikibase_item\",\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        data = safe_get_json(WIKIPEDIA_API, params=params) or {}\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        for page in pages.values():\n",
    "            if \"pageprops\" in page and \"wikibase_item\" in page[\"pageprops\"]:\n",
    "                return page[\"pageprops\"][\"wikibase_item\"]\n",
    "        # Search fallback\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": f\"{language_name} language\",\n",
    "            \"srlimit\": 1,\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "        data = safe_get_json(WIKIPEDIA_API, params=params) or {}\n",
    "        search = data.get(\"query\", {}).get(\"search\", [])\n",
    "        if search:\n",
    "            page_title = search[0][\"title\"]\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"titles\": page_title,\n",
    "                \"prop\": \"pageprops\",\n",
    "                \"ppprop\": \"wikibase_item\",\n",
    "                \"format\": \"json\",\n",
    "            }\n",
    "            data = safe_get_json(WIKIPEDIA_API, params=params) or {}\n",
    "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "            for page in pages.values():\n",
    "                if \"pageprops\" in page and \"wikibase_item\" in page[\"pageprops\"]:\n",
    "                    return page[\"pageprops\"][\"wikibase_item\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting QID for {language_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_parents(entity_id):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?parent ?parentLabel WHERE {{\n",
    "      wd:{entity_id} wdt:P279 ?parent.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    return [(r['parent']['value'].split('/')[-1], r['parentLabel']['value']) for r in results]\n",
    "\n",
    "\n",
    "def get_children_by_p527(entity_id):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?child ?childLabel WHERE {{\n",
    "      wd:{entity_id} wdt:P527 ?child.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    return [(r['child']['value'].split('/')[-1], r['childLabel']['value']) for r in results]\n",
    "\n",
    "\n",
    "def get_children(entity_id):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?child ?childLabel WHERE {{\n",
    "      ?child wdt:P279 wd:{entity_id}.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = safe_get_json(WIKIDATA_SPARQL_ENDPOINT, params={'query': query, 'format': 'json'}) or {}\n",
    "    results = response.get('results', {}).get('bindings', [])\n",
    "    return [(r['child']['value'].split('/')[-1], r['childLabel']['value']) for r in results]\n",
    "\n",
    "\n",
    "def build_language_family_tree(entity_id, depth, current_depth=1, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if current_depth > depth or entity_id in visited:\n",
    "        return []\n",
    "    if len(visited) >= MAX_NODES:\n",
    "        print(\"Max node cap reached; stopping expansion.\")\n",
    "        return []\n",
    "\n",
    "    visited.add(entity_id)\n",
    "    relations: list[tuple[str, str, str]] = []\n",
    "    current_label = get_label(entity_id)\n",
    "\n",
    "    # Parents (superclasses). Filter invalid before using.\n",
    "    parents = get_parents(entity_id)\n",
    "    if entity_id == \"Q1680\":  # debug for English\n",
    "        print(\"Parents of English:\", parents)\n",
    "    for parent_id, parent_label in parents:\n",
    "        if is_valid_language_cached(parent_id):\n",
    "            relations.append((current_label, \"Child of\", parent_label))\n",
    "            relations.extend(build_language_family_tree(parent_id, depth, current_depth + 1, visited))\n",
    "\n",
    "    # Children by P527 (parts/members)\n",
    "    for child_id, child_label in get_children_by_p527(entity_id):\n",
    "        if child_id != entity_id and is_valid_language_cached(child_id):\n",
    "            relations.append((child_label, \"Child of\", current_label))\n",
    "            relations.extend(build_language_family_tree(child_id, depth, current_depth + 1, visited))\n",
    "\n",
    "    # Children by reverse P279 (subclasses)\n",
    "    for child_id, child_label in get_children(entity_id):\n",
    "        if child_id != entity_id and child_id not in visited and is_valid_language_cached(child_id):\n",
    "            relations.append((child_label, \"Child of\", current_label))\n",
    "            relations.extend(build_language_family_tree(child_id, depth, current_depth + 1, visited))\n",
    "\n",
    "    return relations\n",
    "\n",
    "\n",
    "def get_language_family(language_name, depth):\n",
    "    entity_id = get_wikidata_entity_id(language_name)\n",
    "    if not entity_id:\n",
    "        raise ValueError(f\"Language '{language_name}' not found in Wikidata.\")\n",
    "    # Root language is forced valid to ensure at least a starting point\n",
    "    VALID_QIDS.add(entity_id)\n",
    "    relations = build_language_family_tree(entity_id, depth)\n",
    "    unique_relations = list({(r[0], r[1], r[2]) for r in relations})\n",
    "    formatted_relations = [{\"language1\": rel[0], \"relationship\": rel[1], \"language2\": rel[2]} for rel in unique_relations]\n",
    "    return formatted_relations\n",
    "\n",
    "# Example usage\n",
    "family_tree = get_language_family(\"English\", 2)\n",
    "print(f\"Entries: {len(family_tree)}\")\n",
    "print(family_tree[:10])  # preview first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b64300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Reserved cell) – helper / scratch space.\n",
    "# Visualization cells will follow below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91a369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 193, Edges: 214\n",
      "Depth levels computed (sample): [('languages of Ukraine', 0), ('Jewish languages', 0), ('languages of Sweden', 0), ('languages of the Falkland Islands', 0), ('languages of Europe', 0), ('languages of Switzerland', 0), ('languages of Puerto Rico', 0), ('languages of Lebanon', 0), ('Anglo-Frisian', 0), ('languages of Spain', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Build a NetworkX graph from the family_tree list of dicts (language1, relationship, language2)\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    raise SystemExit(\"Please install networkx: pip install networkx\")\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "if 'family_tree' not in globals():\n",
    "    raise RuntimeError(\"family_tree not defined – run the retrieval cell first.\")\n",
    "\n",
    "def build_graph(relations):\n",
    "    G = nx.DiGraph()\n",
    "    for rel in relations:\n",
    "        l1 = rel[\"language1\"]\n",
    "        l2 = rel[\"language2\"]\n",
    "        label = rel[\"relationship\"]\n",
    "        # Edge direction: parent -> child (reverse of 'Child of')\n",
    "        if label == \"Child of\":\n",
    "            parent, child = l2, l1\n",
    "        else:\n",
    "            parent, child = l1, l2\n",
    "        G.add_node(parent)\n",
    "        G.add_node(child)\n",
    "        G.add_edge(parent, child, relationship=label)\n",
    "    return G\n",
    "\n",
    "family_graph = build_graph(family_tree)\n",
    "print(f\"Nodes: {family_graph.number_of_nodes()}, Edges: {family_graph.number_of_edges()}\")\n",
    "\n",
    "# Derive depths (heuristic BFS from nodes with no predecessors)\n",
    "roots = [n for n in family_graph.nodes() if family_graph.in_degree(n) == 0]\n",
    "node_depth = {n: 0 for n in roots}\n",
    "for r in roots:\n",
    "    q = deque([(r, 0)])\n",
    "    while q:\n",
    "        node, d = q.popleft()\n",
    "        for child in family_graph.successors(node):\n",
    "            if child not in node_depth or d + 1 < node_depth[child]:\n",
    "                node_depth[child] = d + 1\n",
    "                q.append((child, d + 1))\n",
    "print(\"Depth levels computed (sample):\", list(node_depth.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b55aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m net = Network(height=\u001b[33m'\u001b[39m\u001b[33m750px\u001b[39m\u001b[33m'\u001b[39m, width=\u001b[33m'\u001b[39m\u001b[33m100\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m, directed=\u001b[38;5;28;01mTrue\u001b[39;00m, notebook=\u001b[38;5;28;01mTrue\u001b[39;00m, bgcolor=\u001b[33m'\u001b[39m\u001b[33m#ffffff\u001b[39m\u001b[33m'\u001b[39m, font_color=\u001b[33m'\u001b[39m\u001b[33m#222222\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m net.toggle_physics(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_options\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43mvar options = \u001b[39;49m\u001b[33;43m{\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m  physics: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m stabilization: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m iterations: 150 }, barnesHut: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m gravitationalConstant: -4000 } },\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m  edges: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m arrows: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m to: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m enabled: true } }, smooth: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m type: \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdynamic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m } }\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m}\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m family_graph.nodes():\n\u001b[32m     25\u001b[39m     depth = \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mnode_depth\u001b[39m\u001b[33m'\u001b[39m, {}).get(node, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyvis\\network.py:1006\u001b[39m, in \u001b[36mNetwork.set_options\u001b[39m\u001b[34m(self, options)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_options\u001b[39m(\u001b[38;5;28mself\u001b[39m, options):\n\u001b[32m    997\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[33;03m    Overrides the default options object passed to the VisJS framework.\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[33;03m    Delegates to the :meth:`options.Options.set` routine.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m \u001b[33;03m    :type options: str\u001b[39;00m\n\u001b[32m   1005\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28mself\u001b[39m.options = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyvis\\options.py:224\u001b[39m, in \u001b[36mOptions.set\u001b[39m\u001b[34m(self, new_options)\u001b[39m\n\u001b[32m    222\u001b[39m first_bracket = options.find(\u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    223\u001b[39m options = options[first_bracket:]\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m options = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m options\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DASUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:353\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \n\u001b[32m    351\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "# Interactive visualization with PyVis (HTML)\n",
    "try:\n",
    "    from pyvis.network import Network\n",
    "except ImportError:\n",
    "    raise SystemExit(\"Please install pyvis: pip install pyvis\")\n",
    "\n",
    "if 'family_graph' not in globals():\n",
    "    raise RuntimeError(\"family_graph not defined – run the graph build cell first.\")\n",
    "\n",
    "palette = [\n",
    "    '#1f77b4', '#2ca02c', '#ff7f0e', '#9467bd', '#8c564b',\n",
    "    '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'\n",
    "]\n",
    "\n",
    "net = Network(height='750px', width='100%', directed=True, notebook=True, bgcolor='#ffffff', font_color='#222222')\n",
    "net.toggle_physics(True)\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  physics: { stabilization: { iterations: 150 }, barnesHut: { gravitationalConstant: -4000 } },\n",
    "  edges: { arrows: { to: { enabled: true } }, smooth: { type: 'dynamic' } }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "for node in family_graph.nodes():\n",
    "    depth = globals().get('node_depth', {}).get(node, 0)\n",
    "    color = palette[depth % len(palette)]\n",
    "    net.add_node(node, label=node, color=color, level=depth)\n",
    "\n",
    "for u, v, data in family_graph.edges(data=True):\n",
    "    rel = data.get('relationship', '')\n",
    "    net.add_edge(u, v, title=rel)\n",
    "\n",
    "net.show('language_family_tree.html')\n",
    "print(\"Generated language_family_tree.html (open in sidebar or a browser).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706fc14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "languages of Ukraine [d=0]\n",
      "└─ English language in Ukraine [d=1]\n",
      "Jewish languages [d=0]\n",
      "└─ Jewish English varieties [d=1]\n",
      "languages of Sweden [d=0]\n",
      "└─ English language in Sweden [d=1]\n",
      "languages of the Falkland Islands [d=0]\n",
      "└─ Falkland Islands English [d=1]\n",
      "languages of Europe [d=0]\n",
      "└─ English language in Europe [d=1]\n",
      "languages of Switzerland [d=0]\n",
      "└─ English language in Switzerland [d=1]\n",
      "languages of Puerto Rico [d=0]\n",
      "└─ English language in Puerto Rico [d=1]\n",
      "languages of Lebanon [d=0]\n",
      "└─ English language in Lebanon [d=1]\n",
      "Anglo-Frisian [d=0]\n",
      "└─ Anglic [d=1]\n",
      "└─ ├─ Yola [d=2]\n",
      "└─ ├─ Middle English [d=2]\n",
      "└─ ├─ ├─ Kentish Dialect [d=3]\n",
      "└─ ├─ ├─ Southern Dialect [d=3]\n",
      "└─ ├─ ├─ Central Middle English [d=3]\n",
      "└─ ├─ ├─ Southern Middle English [d=3]\n",
      "└─ ├─ ├─ East Midland Dialect [d=3]\n",
      "└─ ├─ ├─ Northern Middle English [d=3]\n",
      "└─ ├─ ├─ West Midland Dialect [d=3]\n",
      "└─ ├─ ├─ Late Middle English [d=3]\n",
      "└─ ├─ ├─ Northern Dialect [d=3]\n",
      "└─ ├─ ├─ Midland Middle English [d=3]\n",
      "└─ ├─ ├─ Q1877420 [d=3]\n",
      "└─ ├─ └─ Early Middle English [d=3]\n",
      "└─ ├─ Scots [d=2]\n",
      "└─ ├─ English [d=2]\n",
      "└─ ├─ ├─ Palauan English [d=3]\n",
      "└─ ├─ ├─ Zambian English [d=3]\n",
      "└─ ├─ ├─ English in the Netherlands [d=1]\n",
      "└─ ├─ ├─ Basic English [d=3]\n",
      "└─ ├─ ├─ English as a second or foreign language [d=3]\n",
      "└─ ├─ ├─ African American Vernacular English [d=3]\n",
      "└─ ├─ ├─ ├─ Philadelphia English [d=4]\n",
      "└─ ├─ ├─ ├─ Washington, D.C., African American Vernacular English [d=4]\n",
      "└─ ├─ ├─ ├─ New York dialect [d=4]\n",
      "└─ ├─ ├─ └─ Baltimore dialect [d=4]\n",
      "└─ ├─ ├─ Ugandan English [d=3]\n",
      "└─ ├─ ├─ British English [d=3]\n",
      "└─ ├─ ├─ ├─ English language in England [d=4]\n",
      "└─ ├─ ├─ ├─ Australian English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ South Australian English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Neo-Nyungar [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Western Australian English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Australian Aboriginal English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Standard Australian English [d=4]\n",
      "└─ ├─ ├─ ├─ └─ Strine [d=4]\n",
      "└─ ├─ ├─ ├─ Welsh English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ Port Talbot English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Cardiff English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Abercraf English [d=4]\n",
      "└─ ├─ ├─ ├─ └─ Gower dialect [d=4]\n",
      "└─ ├─ ├─ ├─ Multicultural British English [d=4]\n",
      "└─ ├─ ├─ ├─ received pronunciation [d=3]\n",
      "└─ ├─ ├─ └─ Surrey dialect [d=4]\n",
      "└─ ├─ ├─ International English [d=3]\n",
      "└─ ├─ ├─ └─ Antarctic English [d=4]\n",
      "└─ ├─ ├─ English language in Lebanon [d=1]\n",
      "└─ ├─ ├─ Ozark English [d=3]\n",
      "└─ ├─ ├─ Scottish English [d=3]\n",
      "└─ ├─ ├─ └─ Highland English [d=4]\n",
      "└─ ├─ ├─ Solomon Islands English [d=3]\n",
      "└─ ├─ ├─ Hebronics [d=3]\n",
      "└─ ├─ ├─ Bahamian English [d=3]\n",
      "└─ ├─ ├─ Welsh English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Bay Islands English [d=3]\n",
      "└─ ├─ ├─ Gibraltarian English [d=3]\n",
      "└─ ├─ ├─ Tristan da Cunha English [d=3]\n",
      "└─ ├─ ├─ Trinidadian and Tobagonian English [d=3]\n",
      "└─ ├─ ├─ Tinglish [d=3]\n",
      "└─ ├─ ├─ English language in Algeria [d=1]\n",
      "└─ ├─ ├─ Ghanaian English [d=3]\n",
      "└─ ├─ ├─ South African English [d=3]\n",
      "└─ ├─ ├─ └─ Cape Flats English [d=4]\n",
      "└─ ├─ ├─ Bajan English [d=3]\n",
      "└─ ├─ ├─ Pirate English [d=3]\n",
      "└─ ├─ ├─ Nigerian English [d=3]\n",
      "└─ ├─ ├─ Falkland Islands English [d=1]\n",
      "└─ ├─ ├─ Indian English [d=3]\n",
      "└─ ├─ ├─ └─ Myanmar English [d=4]\n",
      "└─ ├─ ├─ South Atlantic English [d=3]\n",
      "└─ ├─ ├─ Ulster English [d=3]\n",
      "└─ ├─ ├─ English language in Ukraine [d=1]\n",
      "└─ ├─ ├─ English language in Spain [d=1]\n",
      "└─ ├─ ├─ English language in Puerto Rico [d=1]\n",
      "└─ ├─ ├─ Pig Latin [d=3]\n",
      "└─ ├─ ├─ English language in Switzerland [d=1]\n",
      "└─ ├─ ├─ Pennsylvania Dutch English [d=3]\n",
      "└─ ├─ ├─ Luminar English [d=3]\n",
      "└─ ├─ ├─ Ebonics [d=3]\n",
      "└─ ├─ ├─ Buddhist Hybrid English [d=3]\n",
      "└─ ├─ ├─ Solomon Islands Pidgin English [d=3]\n",
      "└─ ├─ ├─ Namlish [d=3]\n",
      "└─ ├─ ├─ English language in Europe [d=1]\n",
      "└─ ├─ ├─ African Nova Scotian English [d=3]\n",
      "└─ ├─ ├─ Gambian English [d=3]\n",
      "└─ ├─ ├─ Newfoundland English [d=3]\n",
      "└─ ├─ ├─ Cameroonian English [d=3]\n",
      "└─ ├─ ├─ North American English [d=3]\n",
      "└─ ├─ ├─ ├─ Pacific Northwest English [d=4]\n",
      "└─ ├─ ├─ ├─ Mexican English [d=4]\n",
      "└─ ├─ ├─ ├─ Bermudian English [d=4]\n",
      "└─ ├─ ├─ ├─ General American English [d=4]\n",
      "└─ ├─ ├─ ├─ Jamaican English [d=3]\n",
      "└─ ├─ ├─ ├─ American English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ Northern American English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ General American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Chicano English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Southern American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ └─ Ozark English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ └─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Philadelphia English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ North–Central American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Older Southern American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Alaskan English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Western Pennsylvania English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Appalachian English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Inland Northern American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Miami English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ California English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New York Latino English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New York dialect [d=4]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ African American Vernacular English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Boontling [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New Orleans English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ African-American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ └─ African American Vernacular English [d=3]\n",
      "└─ ├─ ├─ ├─ ├─ └─   (cycle)\n",
      "└─ ├─ ├─ ├─ ├─ Q20007967 [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Mid-Atlantic accent [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Yooper dialect [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Western American English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New England English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ High Tider [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ New Jersey English [d=4]\n",
      "└─ ├─ ├─ ├─ ├─ Cajun English [d=4]\n",
      "└─ ├─ ├─ ├─ └─ Midland American English [d=4]\n",
      "└─ ├─ ├─ └─ Canadian English [d=3]\n",
      "└─ ├─ ├─ └─ ├─ Quebec English [d=3]\n",
      "└─ ├─ ├─ └─ ├─ Aboriginal English [d=4]\n",
      "└─ ├─ ├─ └─ ├─ Ottawa Valley English [d=4]\n",
      "└─ ├─ ├─ └─ ├─ Standard Canadian English [d=4]\n",
      "└─ ├─ ├─ └─ └─ Multicultural Toronto English [d=4]\n",
      "└─ ├─ ├─ Hawaiian English [d=3]\n",
      "└─ ├─ ├─ Singapore English [d=3]\n",
      "└─ ├─ ├─ South Asian English [d=3]\n",
      "└─ ├─ ├─ ├─ Maldivian English [d=4]\n",
      "└─ ├─ ├─ ├─ Bangladeshi English [d=4]\n",
      "└─ ├─ ├─ ├─ Sri Lankan English [d=4]\n",
      "└─ ├─ ├─ ├─ Indian English [d=3]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ Pakistani English [d=3]\n",
      "└─ ├─ ├─ └─ Nepali English [d=4]\n",
      "└─ ├─ ├─ Zimbabwean English [d=3]\n",
      "└─ ├─ ├─ Malawian English [d=3]\n",
      "└─ ├─ ├─ Butler English [d=3]\n",
      "└─ ├─ ├─ Quebec English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Standard English [d=3]\n",
      "└─ ├─ ├─ ├─ received pronunciation [d=3]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ Standard Australian English [d=4]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ └─ General American English [d=4]\n",
      "└─ ├─ ├─ └─   (cycle)\n",
      "└─ ├─ ├─ New Zealand English [d=3]\n",
      "└─ ├─ ├─ Kenyan English [d=3]\n",
      "└─ ├─ ├─ Northern American English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Singlish [d=3]\n",
      "└─ ├─ ├─ Jewish English varieties [d=1]\n",
      "└─ ├─ ├─ Pakistani English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Southeast Asian English [d=3]\n",
      "└─ ├─ ├─ Philippine English [d=3]\n",
      "└─ ├─ ├─ └─ Englog [d=4]\n",
      "└─ ├─ ├─ └─ └─ Philippine English [d=3]\n",
      "└─ ├─ ├─ └─ └─   (cycle)\n",
      "└─ ├─ ├─ received pronunciation [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Torres Strait English [d=3]\n",
      "└─ ├─ ├─ Iyaric [d=3]\n",
      "└─ ├─ ├─ Euro English [d=3]\n",
      "└─ ├─ ├─ └─ Dunglish [d=4]\n",
      "└─ ├─ ├─ dialects of English [d=3]\n",
      "└─ ├─ ├─ Saint Helena English [d=3]\n",
      "└─ ├─ ├─ Globish [d=3]\n",
      "└─ ├─ ├─ Simplified Technical English [d=3]\n",
      "└─ ├─ ├─ Hiberno-English [d=3]\n",
      "└─ ├─ ├─ ├─ Shelta [d=4]\n",
      "└─ ├─ ├─ ├─ Dublin English [d=4]\n",
      "└─ ├─ ├─ └─ South-West Irish English [d=4]\n",
      "└─ ├─ ├─ Brunei English [d=3]\n",
      "└─ ├─ ├─ Middle English [d=2]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Australian English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Belizean English [d=3]\n",
      "└─ ├─ ├─ Hong Kong English [d=3]\n",
      "└─ ├─ ├─ Denglisch [d=1]\n",
      "└─ ├─ ├─ American English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Zulu English [d=3]\n",
      "└─ ├─ ├─ Simple English [d=3]\n",
      "└─ ├─ ├─ Maltese English [d=3]\n",
      "└─ ├─ ├─ Ghanaian Pidgin English [d=3]\n",
      "└─ ├─ ├─ Canadian English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Channel Island English [d=3]\n",
      "└─ ├─ ├─ Sierra Leonean English [d=3]\n",
      "└─ ├─ ├─ broken English [d=3]\n",
      "└─ ├─ ├─ English language in Sweden [d=1]\n",
      "└─ ├─ ├─ Cyrillic English [d=3]\n",
      "└─ ├─ ├─ Caribbean English [d=3]\n",
      "└─ ├─ ├─ ├─ Gustavia English [d=4]\n",
      "└─ ├─ ├─ ├─ Belizean English [d=3]\n",
      "└─ ├─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ ├─ Cayman Islands English [d=4]\n",
      "└─ ├─ ├─ ├─ Saban English [d=4]\n",
      "└─ ├─ ├─ ├─ Samaná English [d=4]\n",
      "└─ ├─ ├─ └─ Bequia English [d=4]\n",
      "└─ ├─ ├─ Jamaican English [d=3]\n",
      "└─ ├─ ├─   (cycle)\n",
      "└─ ├─ ├─ Manx English [d=3]\n",
      "└─ ├─ └─ Utah Mormon English [d=3]\n",
      "└─ ├─ Modern English [d=2]\n",
      "└─ ├─ Fingalian [d=2]\n",
      "└─ ├─ Early Modern English [d=2]\n",
      "└─ └─ Old English [d=2]\n",
      "languages of Spain [d=0]\n",
      "└─ English language in Spain [d=1]\n",
      "German [d=0]\n",
      "└─ Denglisch [d=1]\n",
      "languages of the Netherlands [d=0]\n",
      "└─ English in the Netherlands [d=1]\n",
      "languages of Algeria [d=0]\n",
      "└─ English language in Algeria [d=1]\n"
     ]
    }
   ],
   "source": [
    "# ASCII tree fallback (choose roots heuristically: nodes with no incoming edges)\n",
    "from collections import defaultdict\n",
    "\n",
    "if 'family_graph' not in globals():\n",
    "    raise RuntimeError(\"family_graph not defined – run the graph build cell first.\")\n",
    "\n",
    "incoming = defaultdict(int)\n",
    "for u, v in family_graph.edges():\n",
    "    incoming[v] += 1\n",
    "\n",
    "roots = [n for n in family_graph.nodes() if incoming[n] == 0] or list(family_graph.nodes())[:1]\n",
    "\n",
    "PRINT_LIMIT = 400  # avoid huge console spam\n",
    "printed = 0\n",
    "\n",
    "def print_tree(node, prefix=\"\", visited=None):\n",
    "    global printed\n",
    "    if printed >= PRINT_LIMIT:\n",
    "        return\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    depth = globals().get('node_depth', {}).get(node, 0)\n",
    "    print(f\"{prefix}{node} [d={depth}]\")\n",
    "    printed += 1\n",
    "    if node in visited:\n",
    "        print(prefix + \"  (cycle)\")\n",
    "        return\n",
    "    visited.add(node)\n",
    "    children = list(family_graph.successors(node))\n",
    "    for i, child in enumerate(children):\n",
    "        if printed >= PRINT_LIMIT:\n",
    "            print(\"... output truncated ...\")\n",
    "            return\n",
    "        is_last = i == len(children) - 1\n",
    "        connector = \"└─ \" if is_last else \"├─ \"\n",
    "        print_tree(child, prefix + connector, visited)\n",
    "\n",
    "for r in roots:\n",
    "    if printed >= PRINT_LIMIT:\n",
    "        break\n",
    "    print_tree(r)\n",
    "if printed >= PRINT_LIMIT:\n",
    "    print(f\"Truncated after {PRINT_LIMIT} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69afe1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True extinct_language\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "class WikidataLanguageValidator:\n",
    "    def __init__(self):\n",
    "        self.sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "        self.valid_types = {\n",
    "            \"language\": \"wd:Q34770\",          # language\n",
    "            \"language_family\": \"wd:Q25295\", # language family\n",
    "            \"dialect\": \"wd:Q33384\"  ,           # dialect\n",
    "            \"extinct_language\":\"wd:Q38058796\",\n",
    "            \"dead_language\":\"wd:Q45762\"\n",
    "        }\n",
    "\n",
    "    def validate_qid(self, qid):\n",
    "        # SPARQL query to check instance of and subclasses for valid types\n",
    "        query = f\"\"\"\n",
    "        SELECT ?type ?typeLabel WHERE {{\n",
    "          VALUES ?item {{ wd:{qid} }} \n",
    "          {{\n",
    "            ?item wdt:P31 ?type.\n",
    "          }} UNION {{\n",
    "            ?item wdt:P279 ?type.\n",
    "          }}\n",
    "          VALUES ?validType {{ { ' '.join(self.valid_types.values())} }}\n",
    "          FILTER(?type IN (?validType))\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sparql.setQuery(query)\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "        results = self.sparql.query().convert()\n",
    "\n",
    "        if results[\"results\"][\"bindings\"]:\n",
    "            type_uri = results[\"results\"][\"bindings\"][0][\"type\"][\"value\"]\n",
    "            type_label = results[\"results\"][\"bindings\"][0][\"typeLabel\"][\"value\"]\n",
    "\n",
    "            for key, val in self.valid_types.items():\n",
    "                if type_uri.endswith(val.split(\":\")[1]):\n",
    "                    return True, key\n",
    "\n",
    "        return False, None\n",
    "\n",
    "# Example usage:\n",
    "validator = WikidataLanguageValidator()\n",
    "valid, classification = validator.validate_qid(\"Q56395\")  # e.g., \"English language\" QID = Q1860\n",
    "print(valid, classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288d9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://commons.wikimedia.org/wiki/Special:FilePath/Anglospeak%20%28subnational%20version%29.svg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "WIKIDATA_SPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "HEADERS = {\"User-Agent\": \"LanguageFamilyTreeBot/1.0 (https://example.com)\"}\n",
    "\n",
    "def get_distribution_map_image(qid):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?image WHERE {{\n",
    "      wd:{qid} wdt:P1846 ?image.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = requests.get(WIKIDATA_SPARQL_ENDPOINT, headers=HEADERS, params={'query': query, 'format': 'json'})\n",
    "    results = response.json().get('results', {}).get('bindings', [])\n",
    "    if results:\n",
    "        # The image URL usually is in the form of a Wikimedia Commons URL or file name\n",
    "        image_url = results[0]['image']['value']\n",
    "        return image_url\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "qid = 'Q1860'  # English language QID\n",
    "image = get_distribution_map_image(qid)\n",
    "print(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730cd126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Infobox Extractor\n",
      "==================================================\n",
      "\n",
      "Extracting information for QID: Q1860\n",
      "Wikidata API error: Expecting value: line 1 column 1 (char 0)\n",
      "{\n",
      "  \"extraction_methods\": [\n",
      "    \"SPARQL\"\n",
      "  ],\n",
      "  \"basic_info\": {},\n",
      "  \"language_family\": {},\n",
      "  \"speakers_info\": {},\n",
      "  \"geographic_info\": {},\n",
      "  \"language_codes\": {},\n",
      "  \"writing_system\": {},\n",
      "  \"official_status\": {},\n",
      "  \"dialects\": {},\n",
      "  \"infobox_fields\": {},\n",
      "  \"all_properties\": {\n",
      "    \"country\": [\n",
      "      \"Samoa\",\n",
      "      \"Solomon Islands\",\n",
      "      \"Vanuatu\",\n",
      "      \"Papua New Guinea\",\n",
      "      \"Palau\",\n",
      "      \"Nauru\",\n",
      "      \"Federated States of Micronesia\",\n",
      "      \"Marshall Islands\",\n",
      "      \"Kiribati\",\n",
      "      \"Fiji\",\n",
      "      \"Guyana\",\n",
      "      \"Trinidad and Tobago\",\n",
      "      \"Saint Vincent and the Grenadines\",\n",
      "      \"Saint Lucia\",\n",
      "      \"Saint Kitts and Nevis\",\n",
      "      \"Jamaica\",\n",
      "      \"Grenada\",\n",
      "      \"The Bahamas\",\n",
      "      \"Antigua and Barbuda\",\n",
      "      \"Dominica\",\n",
      "      \"Jersey\",\n",
      "      \"Israel\",\n",
      "      \"Lebanon\",\n",
      "      \"Maldives\",\n",
      "      \"Malaysia\",\n",
      "      \"Oman\",\n",
      "      \"Pakistan\",\n",
      "      \"Sri Lanka\",\n",
      "      \"United Arab Emirates\",\n",
      "      \"Bhutan\",\n",
      "      \"Brunei\",\n",
      "      \"Tanzania\",\n",
      "      \"Philippines\",\n",
      "      \"Zambia\",\n",
      "      \"Zimbabwe\",\n",
      "      \"South Sudan\",\n",
      "      \"Botswana\",\n",
      "      \"Eritrea\",\n",
      "      \"The Gambia\",\n",
      "      \"Cameroon\",\n",
      "      \"Lesotho\",\n",
      "      \"Liberia\",\n",
      "      \"Madagascar\",\n",
      "      \"Malawi\",\n",
      "      \"Mauritius\",\n",
      "      \"Namibia\",\n",
      "      \"Nigeria\",\n",
      "      \"Uganda\",\n",
      "      \"Rwanda\",\n",
      "      \"Seychelles\",\n",
      "      \"Sierra Leone\",\n",
      "      \"Sudan\",\n",
      "      \"Eswatini\",\n",
      "      \"Gibraltar\",\n",
      "      \"Cayman Islands\",\n",
      "      \"Falkland Islands\",\n",
      "      \"Isle of Man\",\n",
      "      \"United States Virgin Islands\",\n",
      "      \"Montserrat\",\n",
      "      \"American Samoa\",\n",
      "      \"Turks and Caicos Islands\",\n",
      "      \"Aruba\",\n",
      "      \"Bermuda\",\n",
      "      \"Anguilla\",\n",
      "      \"Guernsey\",\n",
      "      \"British Virgin Islands\",\n",
      "      \"Sint Maarten\",\n",
      "      \"Cook Islands\",\n",
      "      \"Niue\",\n",
      "      \"Pitcairn Islands\",\n",
      "      \"Tokelau\",\n",
      "      \"British Indian Ocean Territory\",\n",
      "      \"Saint-Martin\",\n",
      "      \"Saint Helena, Ascension and Tristan da Cunha\"\n",
      "    ],\n",
      "    \"image\": [\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/William%20Shakespeare%20-%20Sonnet%20XXX%20-%20Rapenburg%2030%2C%20Leiden%20%28cropped%29.JPG\"\n",
      "    ],\n",
      "    \"instance of\": [\n",
      "      \"natural language\",\n",
      "      \"macrolanguage\",\n",
      "      \"standard language\",\n",
      "      \"literary language\",\n",
      "      \"modern language\",\n",
      "      \"human language\"\n",
      "    ],\n",
      "    \"audio\": [\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/AuE%20vowel%20length%20sentence.ogg\",\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/Declaration%20of%20Independence%20Part%201.ogg\",\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/For%20example%2C%20in%20many%20places%20women%20vaccinate%20people.ogg\",\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/Recording%20of%20speaker%20of%20British%20English%20%28Received%20Pronunciation%29.ogg\"\n",
      "    ],\n",
      "    \"named after\": [\n",
      "      \"England\"\n",
      "    ],\n",
      "    \"ISO 639-1 code\": [\n",
      "      \"en\"\n",
      "    ],\n",
      "    \"ISO 639-2 code\": [\n",
      "      \"eng\"\n",
      "    ],\n",
      "    \"ISO 639-3 code\": [\n",
      "      \"eng\"\n",
      "    ],\n",
      "    \"GND ID\": [\n",
      "      \"4014777-0\"\n",
      "    ],\n",
      "    \"Library of Congress authority ID\": [\n",
      "      \"sh85043413\"\n",
      "    ],\n",
      "    \"Bibliothèque nationale de France ID\": [\n",
      "      \"119308987\"\n",
      "    ],\n",
      "    \"IdRef ID\": [\n",
      "      \"027219232\"\n",
      "    ],\n",
      "    \"GOST 7.75–97 code\": [\n",
      "      \"анг 045\"\n",
      "    ],\n",
      "    \"subclass of\": [\n",
      "      \"Anglic\"\n",
      "    ],\n",
      "    \"writing system\": [\n",
      "      \"Latin script\",\n",
      "      \"English orthography\"\n",
      "    ],\n",
      "    \"IETF language tag\": [\n",
      "      \"en\"\n",
      "    ],\n",
      "    \"NDL Authority ID\": [\n",
      "      \"00561788\"\n",
      "    ],\n",
      "    \"Commons category\": [\n",
      "      \"English language\"\n",
      "    ],\n",
      "    \"Wikimedia language code\": [\n",
      "      \"en\"\n",
      "    ],\n",
      "    \"said to be the same as\": [\n",
      "      \"Oldspeak\"\n",
      "    ],\n",
      "    \"opposite of\": [\n",
      "      \"non-English\"\n",
      "    ],\n",
      "    \"BNCF Thesaurus ID\": [\n",
      "      \"6546\"\n",
      "    ],\n",
      "    \"has part(s)\": [\n",
      "      \"American English\",\n",
      "      \"British English\",\n",
      "      \"received pronunciation\",\n",
      "      \"African American Vernacular English\",\n",
      "      \"New Zealand English\",\n",
      "      \"Canadian English\",\n",
      "      \"Australian English\",\n",
      "      \"Jamaican English\",\n",
      "      \"Hiberno-English\",\n",
      "      \"Hong Kong English\",\n",
      "      \"South African English\",\n",
      "      \"Indian English\",\n",
      "      \"Scottish English\",\n",
      "      \"Welsh English\",\n",
      "      \"Caribbean English\",\n",
      "      \"Nigerian English\",\n",
      "      \"Quebec English\",\n",
      "      \"Pakistani English\",\n",
      "      \"Euro English\",\n",
      "      \"Belizean English\"\n",
      "    ],\n",
      "    \"Freebase ID\": [\n",
      "      \"/m/02h40lc\"\n",
      "    ],\n",
      "    \"NL CR AUT ID\": [\n",
      "      \"ph114056\"\n",
      "    ],\n",
      "    \"influenced by\": [\n",
      "      \"French\",\n",
      "      \"Greek\",\n",
      "      \"Germanic languages\",\n",
      "      \"Vulgar Latin\",\n",
      "      \"Old Danish\"\n",
      "    ],\n",
      "    \"has edition or translation\": [\n",
      "      \"Two-fisted science : Vannikov the reflector\"\n",
      "    ],\n",
      "    \"IPA transcription\": [\n",
      "      \"ˈɪŋɡlɪʃ\"\n",
      "    ],\n",
      "    \"HDS ID\": [\n",
      "      \"011198\"\n",
      "    ],\n",
      "    \"topic's main category\": [\n",
      "      \"Category:English language\"\n",
      "    ],\n",
      "    \"page banner\": [\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/English%20phrasebook%20WV%20banner.png\"\n",
      "    ],\n",
      "    \"National Library of Israel ID (old)\": [\n",
      "      \"000678210\"\n",
      "    ],\n",
      "    \"National Library of Spain SpMaBN ID (BNE v1.0)\": [\n",
      "      \"XX528236\"\n",
      "    ],\n",
      "    \"spoken text audio\": [\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/Nl-Engels-article.ogg\"\n",
      "    ],\n",
      "    \"Art & Architecture Thesaurus ID\": [\n",
      "      \"300388277\"\n",
      "    ],\n",
      "    \"Dewey Decimal Classification\": [\n",
      "      \"420\"\n",
      "    ],\n",
      "    \"PSH ID\": [\n",
      "      \"6754\"\n",
      "    ],\n",
      "    \"number of speakers, writers, or signers\": [\n",
      "      \"753359540\"\n",
      "    ],\n",
      "    \"topic's main Wikimedia portal\": [\n",
      "      \"Portal:English\"\n",
      "    ],\n",
      "    \"Universal Decimal Classification\": [\n",
      "      \"811.111\"\n",
      "    ],\n",
      "    \"OmegaWiki Defined Meaning\": [\n",
      "      \"5685\"\n",
      "    ],\n",
      "    \"ABS ASCL 2011 code\": [\n",
      "      \"12\",\n",
      "      \"1201\"\n",
      "    ],\n",
      "    \"Gran Enciclopèdia Catalana ID (former scheme)\": [\n",
      "      \"0262155\"\n",
      "    ],\n",
      "    \"described by source\": [\n",
      "      \"Brockhaus and Efron Encyclopedic Dictionary\",\n",
      "      \"Encyclopædia Britannica 11th edition\",\n",
      "      \"Encyclopedic Lexicon\",\n",
      "      \"Granat Encyclopedic Dictionary\",\n",
      "      \"Small Brockhaus and Efron Encyclopedic Dictionary\",\n",
      "      \"Great Soviet Encyclopedia (1926–1947)\",\n",
      "      \"Armenian Soviet Encyclopedia, vol. 1\"\n",
      "    ],\n",
      "    \"National Library of Latvia ID\": [\n",
      "      \"000048547\"\n",
      "    ],\n",
      "    \"Glottolog code\": [\n",
      "      \"stan1293\"\n",
      "    ],\n",
      "    \"Linguasphere code\": [\n",
      "      \"52-ABA\"\n",
      "    ],\n",
      "    \"Encyclopædia Britannica Online ID\": [\n",
      "      \"topic/English-language\"\n",
      "    ],\n",
      "    \"topic has template\": [\n",
      "      \"Q23038502\"\n",
      "    ],\n",
      "    \"WALS lect code\": [\n",
      "      \"eng\"\n",
      "    ],\n",
      "    \"Stack Exchange tag\": [\n",
      "      \"https://linguistics.stackexchange.com/tags/english\"\n",
      "    ],\n",
      "    \"has characteristic\": [\n",
      "      \"agreement\"\n",
      "    ],\n",
      "    \"BBC Things ID\": [\n",
      "      \"03c97771-99ab-478c-b4a8-a0b5c9b07244\"\n",
      "    ],\n",
      "    \"Ethnologue.com language code\": [\n",
      "      \"eng\"\n",
      "    ],\n",
      "    \"native label\": [\n",
      "      \"English\"\n",
      "    ],\n",
      "    \"short name\": [\n",
      "      \"английский\",\n",
      "      \"английски\",\n",
      "      \"إنجليزي\",\n",
      "      \"инглис\",\n",
      "      \"ingiliscə\",\n",
      "      \"инглизсә\",\n",
      "      \"англійская\",\n",
      "      \"ангельская\",\n",
      "      \"Inglis tili\",\n",
      "      \"ағылшынша\",\n",
      "      \"ингилис\",\n",
      "      \"англисаг\",\n",
      "      \"анґліцькый\",\n",
      "      \"англисӣ\",\n",
      "      \"инглисӣ\",\n",
      "      \"англійська\"\n",
      "    ],\n",
      "    \"Global Anabaptist Mennonite Encyclopedia Online ID\": [\n",
      "      \"English Language\"\n",
      "    ],\n",
      "    \"distribution map\": [\n",
      "      \"http://commons.wikimedia.org/wiki/Special:FilePath/Anglospeak%20%28subnational%20version%29.svg\"\n",
      "    ],\n",
      "    \"different from\": [\n",
      "      \"English\",\n",
      "      \"Inglés\",\n",
      "      \"Ingles\",\n",
      "      \"Englisch\",\n",
      "      \"Engelsk\",\n",
      "      \"Ängelsk\"\n",
      "    ],\n",
      "    \"UNESCO language status\": [\n",
      "      \"1 safe\"\n",
      "    ],\n",
      "    \"FAST ID\": [\n",
      "      \"910920\"\n",
      "    ],\n",
      "    \"history of topic\": [\n",
      "      \"history of English\"\n",
      "    ],\n",
      "    \"indigenous to\": [\n",
      "      \"England\"\n",
      "    ],\n",
      "    \"YSO ID\": [\n",
      "      \"2573\"\n",
      "    ],\n",
      "    \"studied by\": [\n",
      "      \"English studies\"\n",
      "    ],\n",
      "    \"BabelNet ID\": [\n",
      "      \"00030862n\"\n",
      "    ],\n",
      "    \"has phoneme\": [\n",
      "      \"open-mid front unrounded vowel\",\n",
      "      \"close back rounded vowel\"\n",
      "    ]\n",
      "  },\n",
      "  \"qid\": \"Q1860\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Any, Optional, List\n",
    "from urllib.parse import quote\n",
    "\n",
    "class LanguageInfoboxExtractor:\n",
    "    \"\"\"\n",
    "    A robust class to extract language information from Wikipedia infoboxes\n",
    "    using QID (Wikidata identifier) with multiple fallback methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
    "        self.wikipedia_api = \"https://en.wikipedia.org/w/api.php\"\n",
    "        self.sparql_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "        \n",
    "    def extract_language_info(self, qid: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main function to extract language information from infobox using QID.\n",
    "        \n",
    "        Args:\n",
    "            qid (str): Wikidata QID (e.g., 'Q1860' for English)\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing extracted language information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Method 1: Try Wikidata API first (most reliable)\n",
    "            wikidata_info = self._get_wikidata_info(qid)\n",
    "            \n",
    "            # Method 2: Get Wikipedia page and extract infobox\n",
    "            wikipedia_info = self._get_wikipedia_infobox(qid)\n",
    "            \n",
    "            # Method 3: Use SPARQL for comprehensive data\n",
    "            sparql_info = self._get_sparql_info(qid)\n",
    "            \n",
    "            # Merge all information with priority: SPARQL > Wikidata > Wikipedia\n",
    "            merged_info = self._merge_info(sparql_info, wikidata_info, wikipedia_info)\n",
    "            merged_info['qid'] = qid\n",
    "            \n",
    "            return merged_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to extract information for {qid}: {str(e)}\"}\n",
    "    \n",
    "    def _get_wikidata_info(self, qid: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract language information using Wikidata API.\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'action': 'wbgetentities',\n",
    "                'format': 'json',\n",
    "                'ids': qid,\n",
    "                'props': 'labels|descriptions|claims|sitelinks',\n",
    "                'languages': 'en',\n",
    "                'sitefilter': 'enwiki'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikidata_api, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'entities' not in data or qid not in data['entities']:\n",
    "                return {}\n",
    "                \n",
    "            entity = data['entities'][qid]\n",
    "            \n",
    "            # Extract relevant language properties\n",
    "            info = {\n",
    "                'name': entity.get('labels', {}).get('en', {}).get('value', ''),\n",
    "                'description': entity.get('descriptions', {}).get('en', {}).get('value', ''),\n",
    "                'wikipedia_url': self._get_wikipedia_url(entity.get('sitelinks', {})),\n",
    "            }\n",
    "            \n",
    "            # Extract claims for language-specific properties\n",
    "            claims = entity.get('claims', {})\n",
    "            \n",
    "            # Map common language properties based on Template:Infobox language\n",
    "            property_mapping = {\n",
    "                'P31': 'instance_of',           # Instance of\n",
    "                'P279': 'subclass_of',          # Subclass of  \n",
    "                'P17': 'country',               # Country/states\n",
    "                'P495': 'country_of_origin',    # Country of origin\n",
    "                'P2341': 'indigenous_to',       # Indigenous to/region\n",
    "                'P1098': 'speakers',            # Number of speakers\n",
    "                'P1999': 'pronunciation',       # Pronunciation\n",
    "                'P1705': 'native_name',         # Native name\n",
    "                'P138': 'named_after',          # Named after\n",
    "                'P361': 'part_of',              # Part of (language family)\n",
    "                'P527': 'has_parts',            # Has parts (dialects)\n",
    "                'P155': 'follows',              # Follows (early forms)\n",
    "                'P156': 'followed_by',          # Followed by\n",
    "                'P460': 'said_to_be_same_as',   # Said to be same as\n",
    "                'P1412': 'languages_spoken',    # Languages spoken\n",
    "                'P103': 'native_language',      # Native language\n",
    "                'P282': 'writing_system',       # Writing system/script\n",
    "                'P37': 'official_language_in',  # Official language in\n",
    "                'P1885': 'minority_language_in', # Minority language in\n",
    "                'P1018': 'language_regulatory_body', # Regulated by/agency\n",
    "                'P219': 'iso_639_1',           # ISO 639-1 code\n",
    "                'P218': 'iso_639_2',           # ISO 639-2 code  \n",
    "                'P220': 'iso_639_3',           # ISO 639-3 code\n",
    "                'P1394': 'linguist_list',      # Linguist List code\n",
    "                'P1233': 'glottolog',          # Glottolog code\n",
    "                'P3823': 'aiatsis',            # AIATSIS code\n",
    "                'P1216': 'guthrie_code',       # Guthrie code\n",
    "                'P3133': 'linguasphere',       # Linguasphere code\n",
    "                'P5755': 'ietf_tag',           # IETF language tag\n",
    "                'P625': 'coordinates',          # Geographic coordinates\n",
    "                'P18': 'image',                 # Image\n",
    "                'P242': 'locator_map',          # Locator map\n",
    "            }\n",
    "            \n",
    "            for prop_id, prop_name in property_mapping.items():\n",
    "                if prop_id in claims:\n",
    "                    info[prop_name] = self._extract_claim_values(claims[prop_id])\n",
    "            \n",
    "            return info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Wikidata API error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _get_wikipedia_infobox(self, qid: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract infobox from Wikipedia page using QID.\"\"\"\n",
    "        try:\n",
    "            # First get Wikipedia page title from QID\n",
    "            page_title = self._get_wikipedia_title(qid)\n",
    "            if not page_title:\n",
    "                return {}\n",
    "                \n",
    "            # Get page content with infobox\n",
    "            params = {\n",
    "                'action': 'parse',\n",
    "                'format': 'json',\n",
    "                'page': page_title,\n",
    "                'prop': 'wikitext',\n",
    "                'section': 0\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikipedia_api, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'parse' not in data or 'wikitext' not in data['parse']:\n",
    "                return {}\n",
    "                \n",
    "            wikitext = data['parse']['wikitext']['*']\n",
    "            \n",
    "            # Extract infobox using regex\n",
    "            infobox_data = self._parse_infobox(wikitext)\n",
    "            \n",
    "            return infobox_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia infobox error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _get_sparql_info(self, qid: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive language information using SPARQL.\"\"\"\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "            \n",
    "            SELECT ?property ?propertyLabel ?value ?valueLabel WHERE {{\n",
    "              wd:{qid} ?predicate ?value .\n",
    "              ?property wikibase:directClaim ?predicate .\n",
    "              FILTER(?predicate != wdt:P31 || ?value != wd:Q34770)\n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "            }}\n",
    "            LIMIT 200\n",
    "            \"\"\"\n",
    "            \n",
    "            headers = {\n",
    "                'Accept': 'application/sparql-results+json',\n",
    "                'User-Agent': 'Language-Infobox-Extractor/1.0'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(\n",
    "                self.sparql_endpoint,\n",
    "                params={'query': query},\n",
    "                headers=headers,\n",
    "                timeout=15\n",
    "            )\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'results' not in data or 'bindings' not in data['results']:\n",
    "                return {}\n",
    "                \n",
    "            sparql_info = {}\n",
    "            for binding in data['results']['bindings']:\n",
    "                prop_label = binding.get('propertyLabel', {}).get('value', '')\n",
    "                value_label = binding.get('valueLabel', {}).get('value', '')\n",
    "                \n",
    "                if prop_label and value_label:\n",
    "                    if prop_label not in sparql_info:\n",
    "                        sparql_info[prop_label] = []\n",
    "                    if value_label not in sparql_info[prop_label]:\n",
    "                        sparql_info[prop_label].append(value_label)\n",
    "            \n",
    "            return sparql_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"SPARQL error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _get_wikipedia_url(self, sitelinks: Dict) -> str:\n",
    "        \"\"\"Extract Wikipedia URL from sitelinks.\"\"\"\n",
    "        if 'enwiki' in sitelinks:\n",
    "            title = sitelinks['enwiki']['title']\n",
    "            return f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        return \"\"\n",
    "    \n",
    "    def _get_wikipedia_title(self, qid: str) -> Optional[str]:\n",
    "        \"\"\"Get Wikipedia page title from QID.\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'action': 'wbgetentities',\n",
    "                'format': 'json',\n",
    "                'ids': qid,\n",
    "                'props': 'sitelinks',\n",
    "                'sitefilter': 'enwiki'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.wikidata_api, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if ('entities' in data and qid in data['entities'] and \n",
    "                'sitelinks' in data['entities'][qid] and \n",
    "                'enwiki' in data['entities'][qid]['sitelinks']):\n",
    "                return data['entities'][qid]['sitelinks']['enwiki']['title']\n",
    "                \n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def _extract_claim_values(self, claims: List) -> List[str]:\n",
    "        \"\"\"Extract values from Wikidata claims.\"\"\"\n",
    "        values = []\n",
    "        for claim in claims:\n",
    "            try:\n",
    "                if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
    "                    datavalue = claim['mainsnak']['datavalue']\n",
    "                    if datavalue['type'] == 'wikibase-entityid':\n",
    "                        # For entity references, we'd need another API call to get labels\n",
    "                        entity_id = datavalue['value']['id']\n",
    "                        values.append(f\"wd:{entity_id}\")\n",
    "                    elif datavalue['type'] == 'string':\n",
    "                        values.append(datavalue['value'])\n",
    "                    elif datavalue['type'] == 'quantity':\n",
    "                        values.append(str(datavalue['value']['amount']))\n",
    "                    elif datavalue['type'] == 'time':\n",
    "                        values.append(datavalue['value']['time'])\n",
    "            except Exception:\n",
    "                continue\n",
    "        return values\n",
    "    \n",
    "    def _parse_infobox(self, wikitext: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse infobox from Wikipedia wikitext.\"\"\"\n",
    "        infobox_info = {}\n",
    "        \n",
    "        # Look for infobox language template\n",
    "        infobox_pattern = r'{{\\\\s*[Ii]nfobox\\\\s+[Ll]anguage[^}]*?}}'\n",
    "        match = re.search(infobox_pattern, wikitext, re.DOTALL)\n",
    "        \n",
    "        if not match:\n",
    "            return {}\n",
    "            \n",
    "        infobox_text = match.group(0)\n",
    "        \n",
    "        # Extract parameters using more robust regex\n",
    "        lines = infobox_text.split('\\n')\n",
    "        current_param = None\n",
    "        current_value = \"\"\n",
    "        \n",
    "        for line in lines[1:-1]:  # Skip first {{ and last }}\n",
    "            line = line.strip()\n",
    "            if line.startswith('|'):\n",
    "                # Save previous parameter\n",
    "                if current_param:\n",
    "                    infobox_info[current_param.lower().strip()] = current_value.strip()\n",
    "                \n",
    "                # Start new parameter\n",
    "                if '=' in line:\n",
    "                    param, value = line[1:].split('=', 1)\n",
    "                    current_param = param.strip()\n",
    "                    current_value = value.strip()\n",
    "                else:\n",
    "                    current_param = line[1:].strip()\n",
    "                    current_value = \"\"\n",
    "            else:\n",
    "                # Continue current parameter value\n",
    "                if current_param:\n",
    "                    current_value += \" \" + line\n",
    "        \n",
    "        # Save last parameter\n",
    "        if current_param:\n",
    "            infobox_info[current_param.lower().strip()] = current_value.strip()\n",
    "        \n",
    "        # Clean up values\n",
    "        for key, value in infobox_info.items():\n",
    "            # Remove templates and links\n",
    "            value = re.sub(r'{{[^}]*}}', '', value)\n",
    "            value = re.sub(r'\\\\[\\\\[[^\\\\]]*\\\\]\\\\]', '', value)\n",
    "            value = re.sub(r'\\\\s+', ' ', value)\n",
    "            infobox_info[key] = value.strip()\n",
    "        \n",
    "        return infobox_info\n",
    "    \n",
    "    def _merge_info(self, sparql_info: Dict, wikidata_info: Dict, wikipedia_info: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Merge information from all sources with structured output.\"\"\"\n",
    "        merged = {\n",
    "            'extraction_methods': [],\n",
    "            'basic_info': {},\n",
    "            'language_family': {},\n",
    "            'speakers_info': {},\n",
    "            'geographic_info': {},\n",
    "            'language_codes': {},\n",
    "            'writing_system': {},\n",
    "            'official_status': {},\n",
    "            'dialects': {},\n",
    "            'infobox_fields': {},\n",
    "            'all_properties': {}\n",
    "        }\n",
    "        \n",
    "        # Track which methods provided data\n",
    "        if sparql_info:\n",
    "            merged['extraction_methods'].append('SPARQL')\n",
    "        if wikidata_info:\n",
    "            merged['extraction_methods'].append('Wikidata API')\n",
    "        if wikipedia_info:\n",
    "            merged['extraction_methods'].append('Wikipedia infobox')\n",
    "        \n",
    "        # Merge basic information\n",
    "        if wikidata_info:\n",
    "            merged['basic_info'] = {\n",
    "                'name': wikidata_info.get('name', ''),\n",
    "                'native_name': wikidata_info.get('native_name', []),\n",
    "                'description': wikidata_info.get('description', ''),\n",
    "                'wikipedia_url': wikidata_info.get('wikipedia_url', '')\n",
    "            }\n",
    "            \n",
    "            # Categorize information\n",
    "            merged['language_family'] = {\n",
    "                'part_of': wikidata_info.get('part_of', []),\n",
    "                'has_parts': wikidata_info.get('has_parts', [])\n",
    "            }\n",
    "            \n",
    "            merged['speakers_info'] = {\n",
    "                'speakers': wikidata_info.get('speakers', [])\n",
    "            }\n",
    "            \n",
    "            merged['geographic_info'] = {\n",
    "                'country': wikidata_info.get('country', []),\n",
    "                'indigenous_to': wikidata_info.get('indigenous_to', []),\n",
    "                'coordinates': wikidata_info.get('coordinates', [])\n",
    "            }\n",
    "            \n",
    "            merged['language_codes'] = {\n",
    "                'iso_639_1': wikidata_info.get('iso_639_1', []),\n",
    "                'iso_639_2': wikidata_info.get('iso_639_2', []),\n",
    "                'iso_639_3': wikidata_info.get('iso_639_3', []),\n",
    "                'glottolog': wikidata_info.get('glottolog', []),\n",
    "                'linguist_list': wikidata_info.get('linguist_list', []),\n",
    "                'linguasphere': wikidata_info.get('linguasphere', []),\n",
    "                'ietf_tag': wikidata_info.get('ietf_tag', [])\n",
    "            }\n",
    "            \n",
    "            merged['writing_system'] = {\n",
    "                'script': wikidata_info.get('writing_system', [])\n",
    "            }\n",
    "            \n",
    "            merged['official_status'] = {\n",
    "                'official_in': wikidata_info.get('official_language_in', []),\n",
    "                'minority_in': wikidata_info.get('minority_language_in', []),\n",
    "                'regulated_by': wikidata_info.get('language_regulatory_body', [])\n",
    "            }\n",
    "        \n",
    "        # Add Wikipedia infobox raw data\n",
    "        if wikipedia_info:\n",
    "            merged['infobox_fields'] = wikipedia_info\n",
    "        \n",
    "        # Add all SPARQL properties\n",
    "        if sparql_info:\n",
    "            merged['all_properties'] = sparql_info\n",
    "        \n",
    "        return merged\n",
    "\n",
    "# Convenience function for easy usage\n",
    "def extract_language_info_by_qid(qid: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract language information from Wikipedia infobox using QID.\n",
    "    \n",
    "    Args:\n",
    "        qid (str): Wikidata QID (e.g., 'Q1860' for English, 'Q150' for French)\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing comprehensive language information\n",
    "        \n",
    "    Example:\n",
    "        >>> info = extract_language_info_by_qid('Q1860')  # English\n",
    "        >>> print(info['basic_info']['name'])\n",
    "        'English'\n",
    "        >>> print(info['language_codes']['iso_639_1'])\n",
    "        ['en']\n",
    "    \"\"\"\n",
    "    extractor = LanguageInfoboxExtractor()\n",
    "    return extractor.extract_language_info(qid)\n",
    "\n",
    "# Helper function to get QID from language name\n",
    "def get_qid_from_language_name(language_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get QID from language name using Wikidata search.\n",
    "    \n",
    "    Args:\n",
    "        language_name (str): Name of the language (e.g., 'English', 'French')\n",
    "        \n",
    "    Returns:\n",
    "        QID string if found, None otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "        params = {\n",
    "            'action': 'wbsearchentities',\n",
    "            'format': 'json',\n",
    "            'language': 'en',\n",
    "            'type': 'item',\n",
    "            'search': language_name,\n",
    "            'limit': 5\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'search' in data and data['search']:\n",
    "            # Look for items that are instances of language or natural language\n",
    "            for item in data['search']:\n",
    "                description = item.get('description', '').lower()\n",
    "                if any(keyword in description for keyword in ['language', 'linguistic']):\n",
    "                    return item['id']\n",
    "                    \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching for QID: {e}\")\n",
    "        return None\n",
    "\n",
    "# Testing function\n",
    "def test_extraction():\n",
    "    \"\"\"Test the extraction with common language QIDs.\"\"\"\n",
    "    test_cases = {\n",
    "        'Q1860': 'English',\n",
    "        'Q150': 'French', \n",
    "        'Q188': 'German',\n",
    "        'Q1321': 'Spanish',\n",
    "        'Q7737': 'Russian',\n",
    "        'Q9058': 'Chinese',\n",
    "        'Q5287': 'Japanese'\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for qid, lang_name in test_cases.items():\n",
    "        print(f\"Testing {lang_name} ({qid})...\")\n",
    "        try:\n",
    "            info = extract_language_info_by_qid(qid)\n",
    "            if 'error' not in info:\n",
    "                print(f\"✓ Successfully extracted data for {lang_name}\")\n",
    "                print(f\"  Methods used: {', '.join(info.get('extraction_methods', []))}\")\n",
    "                print(f\"  Basic info: {info.get('basic_info', {}).get('name', 'Unknown')}\")\n",
    "            else:\n",
    "                print(f\"✗ Error for {lang_name}: {info['error']}\")\n",
    "            results[qid] = info\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to extract {lang_name}: {e}\")\n",
    "            results[qid] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    print(\"Language Infobox Extractor\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test with English\n",
    "    qid = \"Q1860\"  # English\n",
    "    print(f\"\\nExtracting information for QID: {qid}\")\n",
    "    \n",
    "    try:\n",
    "        language_info = extract_language_info_by_qid(qid)\n",
    "        print(json.dumps(language_info, indent=2, ensure_ascii=False))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Optional: Run tests for multiple languages\n",
    "    # print(\"\\n\" + \"=\" * 50)\n",
    "    # print(\"Running tests for multiple languages...\")\n",
    "    # test_results = test_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26dd7968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Language Infobox Extractor\n",
      "==================================================\n",
      "\\nExtracting English language infobox (Q1860)...\n",
      "✗ Error: No Wikipedia page found for QID: Q1860\n",
      "\\n==================================================\n",
      "Some supported Template:Infobox language fields:\n",
      "  name: Language name\n",
      "  nativename: Native name\n",
      "  speakers: Number of native speakers\n",
      "  family: Language family description\n",
      "  script: Writing system\n",
      "  iso1: ISO 639-1 code\n",
      "  iso3: ISO 639-3 code\n",
      "\\n... and 121 more fields supported\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from typing import Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "class WikipediaInfoboxExtractor:\n",
    "    \"\"\"\n",
    "    A focused class to extract language information specifically from Wikipedia infoboxes.\n",
    "    Uses only Wikipedia API and infobox parsing - no Wikidata or SPARQL.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wikipedia_api = \"https://en.wikipedia.org/w/api.php\"\n",
    "        \n",
    "    def extract_infobox_by_qid(self, qid: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract infobox data using Wikidata QID to find the Wikipedia page.\n",
    "        \n",
    "        Args:\n",
    "            qid (str): Wikidata QID (e.g., 'Q1860' for English)\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing parsed infobox fields\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get Wikipedia page title from QID\n",
    "            page_title = self._get_wikipedia_title_from_qid(qid)\n",
    "            if not page_title:\n",
    "                return {\"error\": f\"No Wikipedia page found for QID: {qid}\"}\n",
    "            \n",
    "            return self.extract_infobox_by_title(page_title)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to extract infobox for {qid}: {str(e)}\"}\n",
    "    \n",
    "    def extract_infobox_by_title(self, page_title: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract infobox data from Wikipedia page by title.\n",
    "        \n",
    "        Args:\n",
    "            page_title (str): Wikipedia page title (e.g., 'English language')\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing parsed infobox fields\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get page wikitext\n",
    "            wikitext = self._get_page_wikitext(page_title)\n",
    "            if not wikitext:\n",
    "                return {\"error\": f\"Could not retrieve page content for: {page_title}\"}\n",
    "            \n",
    "            # Extract and parse infobox\n",
    "            infobox_data = self._parse_language_infobox(wikitext)\n",
    "            \n",
    "            if not infobox_data:\n",
    "                return {\"error\": f\"No language infobox found on page: {page_title}\"}\n",
    "            \n",
    "            # Add metadata\n",
    "            result = {\n",
    "                \"page_title\": page_title,\n",
    "                \"wikipedia_url\": f\"https://en.wikipedia.org/wiki/{page_title.replace(' ', '_')}\",\n",
    "                \"infobox_type\": \"language\",\n",
    "                \"fields\": infobox_data,\n",
    "                \"field_count\": len(infobox_data)\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to extract infobox from {page_title}: {str(e)}\"}\n",
    "    \n",
    "    def _get_wikipedia_title_from_qid(self, qid: str) -> Optional[str]:\n",
    "        \"\"\"Get Wikipedia page title from Wikidata QID.\"\"\"\n",
    "        try:\n",
    "            wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
    "            params = {\n",
    "                'action': 'wbgetentities',\n",
    "                'format': 'json',\n",
    "                'ids': qid,\n",
    "                'props': 'sitelinks',\n",
    "                'sitefilter': 'enwiki'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(wikidata_api, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if ('entities' in data and qid in data['entities'] and \n",
    "                'sitelinks' in data['entities'][qid] and \n",
    "                'enwiki' in data['entities'][qid]['sitelinks']):\n",
    "                return data['entities'][qid]['sitelinks']['enwiki']['title']\n",
    "                \n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def _get_page_wikitext(self, page_title: str) -> Optional[str]:\n",
    "        \"\"\"Get raw wikitext from Wikipedia page.\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'action': 'parse',\n",
    "                'format': 'json',\n",
    "                'page': page_title,\n",
    "                'prop': 'wikitext',\n",
    "                'section': 0  # Only get the first section where infobox usually is\n",
    "            }\n",
    "            HEADERS = {\"User-Agent\": \"LanguageFamilyTreeService/1.0 (https://example.com)\", \"Accept\": \"application/json\"}\n",
    "    \n",
    "            response = requests.get(self.wikipedia_api, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'parse' in data and 'wikitext' in data['parse']:\n",
    "                return data['parse']['wikitext']['*']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting wikitext: {e}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def _parse_language_infobox(self, wikitext: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Parse Template:Infobox language from wikitext.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with infobox field names as keys and values as strings\n",
    "        \"\"\"\n",
    "        # Look for infobox language template - case insensitive\n",
    "        infobox_pattern = r'{{\\\\s*[Ii]nfobox\\\\s+[Ll]anguage\\\\s*([^{}]*(?:{[^{}]*}[^{}]*)*)}}'\n",
    "        \n",
    "        match = re.search(infobox_pattern, wikitext, re.DOTALL)\n",
    "        if not match:\n",
    "            return {}\n",
    "        \n",
    "        infobox_content = match.group(1)\n",
    "        \n",
    "        # Parse parameters using a more robust approach\n",
    "        fields = {}\n",
    "        \n",
    "        # Split by | but be careful about nested templates and links\n",
    "        parts = self._split_infobox_params(infobox_content)\n",
    "        \n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if '=' in part:\n",
    "                key, value = part.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                \n",
    "                if key and value:\n",
    "                    # Clean the value\n",
    "                    cleaned_value = self._clean_infobox_value(value)\n",
    "                    if cleaned_value:  # Only add non-empty values\n",
    "                        fields[key] = cleaned_value\n",
    "        \n",
    "        return fields\n",
    "    \n",
    "    def _split_infobox_params(self, content: str) -> list:\n",
    "        \"\"\"\n",
    "        Split infobox content by | while respecting nested templates and links.\n",
    "        \"\"\"\n",
    "        parts = []\n",
    "        current_part = \"\"\n",
    "        brace_depth = 0\n",
    "        bracket_depth = 0\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(content):\n",
    "            char = content[i]\n",
    "            \n",
    "            if char == '{':\n",
    "                if i + 1 < len(content) and content[i + 1] == '{':\n",
    "                    brace_depth += 1\n",
    "                    current_part += '{{'\n",
    "                    i += 2\n",
    "                    continue\n",
    "            elif char == '}':\n",
    "                if i + 1 < len(content) and content[i + 1] == '}':\n",
    "                    brace_depth -= 1\n",
    "                    current_part += '}}'\n",
    "                    i += 2\n",
    "                    continue\n",
    "            elif char == '[':\n",
    "                if i + 1 < len(content) and content[i + 1] == '[':\n",
    "                    bracket_depth += 1\n",
    "                    current_part += '[['\n",
    "                    i += 2\n",
    "                    continue\n",
    "            elif char == ']':\n",
    "                if i + 1 < len(content) and content[i + 1] == ']':\n",
    "                    bracket_depth -= 1\n",
    "                    current_part += ']]'\n",
    "                    i += 2\n",
    "                    continue\n",
    "            elif char == '|' and brace_depth == 0 and bracket_depth == 0:\n",
    "                if current_part.strip():\n",
    "                    parts.append(current_part.strip())\n",
    "                current_part = \"\"\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            current_part += char\n",
    "            i += 1\n",
    "        \n",
    "        # Add the last part\n",
    "        if current_part.strip():\n",
    "            parts.append(current_part.strip())\n",
    "        \n",
    "        return parts\n",
    "    \n",
    "    def _clean_infobox_value(self, value: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean infobox value by removing wiki markup but preserving content.\n",
    "        \"\"\"\n",
    "        # Remove comments\n",
    "        value = re.sub(r'<!--.*?-->', '', value, flags=re.DOTALL)\n",
    "        \n",
    "        # Handle templates - remove simple ones, extract content from others\n",
    "        def replace_template(match):\n",
    "            template_content = match.group(1)\n",
    "            # For simple formatting templates, try to extract useful content\n",
    "            if '|' in template_content:\n",
    "                parts = template_content.split('|')\n",
    "                # Return the last part which is usually the display text\n",
    "                return parts[-1].strip()\n",
    "            return ''\n",
    "        \n",
    "        value = re.sub(r'{{([^{}]+)}}', replace_template, value)\n",
    "        \n",
    "        # Handle links - extract display text or target\n",
    "        def replace_link(match):\n",
    "            link_content = match.group(1)\n",
    "            if '|' in link_content:\n",
    "                # [[target|display text]] -> display text\n",
    "                return link_content.split('|')[-1].strip()\n",
    "            else:\n",
    "                # [[target]] -> target\n",
    "                return link_content.strip()\n",
    "        \n",
    "        value = re.sub(r'\\\\[\\\\[([^\\\\]]+)\\\\]\\\\]', replace_link, value)\n",
    "        \n",
    "        # Remove remaining markup\n",
    "        value = re.sub(r\"'{2,}\", '', value)  # Remove bold/italic markup\n",
    "        value = re.sub(r'<[^>]+>', '', value)  # Remove HTML tags\n",
    "        value = re.sub(r'\\\\n+', ' ', value)  # Replace newlines with spaces\n",
    "        value = re.sub(r'\\\\s+', ' ', value)  # Normalize whitespace\n",
    "        \n",
    "        return value.strip()\n",
    "    \n",
    "    def get_infobox_template_fields(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Return a mapping of common Template:Infobox language fields and their descriptions.\n",
    "        Based on the official documentation.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Basic information\n",
    "            \"name\": \"Language name\",\n",
    "            \"altname\": \"Alternative name\",\n",
    "            \"nativename\": \"Native name\",\n",
    "            \"pronunciation\": \"Pronunciation in IPA\",\n",
    "            \"acceptance\": \"Questionable acceptance/status\",\n",
    "            \n",
    "            # Geographic and demographic\n",
    "            \"states\": \"Countries where mainly spoken\",\n",
    "            \"state\": \"Alias for states\", \n",
    "            \"region\": \"Geographic regions\",\n",
    "            \"ethnicity\": \"Associated ethnic groups\",\n",
    "            \"speakers\": \"Number of native speakers\",\n",
    "            \"date\": \"Date of speaker estimate\",\n",
    "            \"dateprefix\": \"Text before date\",\n",
    "            \"ref\": \"Reference for speaker data\",\n",
    "            \"speakers2\": \"Second line of speaker data\",\n",
    "            \"era\": \"Era of use (historical languages)\",\n",
    "            \"extinct\": \"Date/info about extinction\",\n",
    "            \"revived\": \"Revival information\",\n",
    "            \n",
    "            # Language classification\n",
    "            \"familycolor\": \"Language family color code\",\n",
    "            \"family\": \"Language family description\",\n",
    "            \"fam1\": \"Broadest language family\",\n",
    "            \"fam2\": \"More specific subfamily\", \n",
    "            \"fam3\": \"Even more specific group\",\n",
    "            \"fam4\": \"Fourth level classification\",\n",
    "            \"fam5\": \"Fifth level classification\",\n",
    "            \"fam6\": \"Sixth level classification\",\n",
    "            \"fam7\": \"Seventh level classification\",\n",
    "            \"fam8\": \"Eighth level classification\",\n",
    "            \"fam9\": \"Ninth level classification\",\n",
    "            \"fam10\": \"Tenth level classification\",\n",
    "            \"fam11\": \"Eleventh level classification\",\n",
    "            \"fam12\": \"Twelfth level classification\",\n",
    "            \"fam13\": \"Thirteenth level classification\",\n",
    "            \"fam14\": \"Fourteenth level classification\",\n",
    "            \"fam15\": \"Fifteenth level classification\",\n",
    "            \"ancestor\": \"Ancestral form\",\n",
    "            \"ancestor2\": \"Second ancestral form\",\n",
    "            \"ancestor3\": \"Third ancestral form\",\n",
    "            \"ancestor4\": \"Fourth ancestral form\",\n",
    "            \"ancestor5\": \"Fifth ancestral form\",\n",
    "            \"ancestor6\": \"Sixth ancestral form\",\n",
    "            \"ancestor7\": \"Seventh ancestral form\",\n",
    "            \"ancestor8\": \"Eighth ancestral form\",\n",
    "            \"protoname\": \"Proto-language name\",\n",
    "            \n",
    "            # Dialects and standards\n",
    "            \"dialects\": \"Dialect information\",\n",
    "            \"dia1\": \"First dialect\",\n",
    "            \"dia2\": \"Second dialect\",\n",
    "            \"dia3\": \"Third dialect\",\n",
    "            \"dia4\": \"Fourth dialect\",\n",
    "            \"dia5\": \"Fifth dialect\",\n",
    "            \"listclass\": \"List class for dialects\",\n",
    "            \"standards\": \"Standard forms\",\n",
    "            \"stand1\": \"First standard\",\n",
    "            \"stand2\": \"Second standard\",\n",
    "            \"stand3\": \"Third standard\",\n",
    "            \"stand4\": \"Fourth standard\",\n",
    "            \"stand5\": \"Fifth standard\",\n",
    "            \"stand6\": \"Sixth standard\",\n",
    "            \n",
    "            # Writing and communication\n",
    "            \"script\": \"Writing system\",\n",
    "            \"sign\": \"Sign language forms\",\n",
    "            \"posteriori\": \"A posteriori sources (conlangs)\",\n",
    "            \n",
    "            # Official status\n",
    "            \"nation\": \"Countries where official\",\n",
    "            \"minority\": \"Countries where minority language\",\n",
    "            \"agency\": \"Regulatory body\",\n",
    "            \"development_body\": \"Development organization\",\n",
    "            \n",
    "            # Language codes\n",
    "            \"iso1\": \"ISO 639-1 code\",\n",
    "            \"iso1comment\": \"ISO 639-1 comment\",\n",
    "            \"iso2\": \"ISO 639-2 code\",\n",
    "            \"iso2b\": \"ISO 639-2 bibliographic code\",\n",
    "            \"iso2t\": \"ISO 639-2 terminological code\",\n",
    "            \"iso2comment\": \"ISO 639-2 comment\",\n",
    "            \"iso3\": \"ISO 639-3 code\",\n",
    "            \"iso3comment\": \"ISO 639-3 comment\",\n",
    "            \"iso6\": \"ISO 639-6 code\",\n",
    "            \"isoexception\": \"ISO exception type\",\n",
    "            \"lc1\": \"First dialect ISO code\",\n",
    "            \"ld1\": \"First dialect name\",\n",
    "            \"lc2\": \"Second dialect ISO code\",\n",
    "            \"ld2\": \"Second dialect name\",\n",
    "            \"linglist\": \"Linguist List code\",\n",
    "            \"lingname\": \"Linguist List name\",\n",
    "            \"linglist2\": \"Second Linguist List code\",\n",
    "            \"lingname2\": \"Second Linguist List name\",\n",
    "            \"glotto\": \"Glottolog code\",\n",
    "            \"glottorefname\": \"Glottolog reference name\",\n",
    "            \"glotto2\": \"Second Glottolog code\",\n",
    "            \"glottorefname2\": \"Second Glottolog reference name\",\n",
    "            \"aiatsis\": \"AIATSIS code\",\n",
    "            \"aiatsisname\": \"AIATSIS name\",\n",
    "            \"aiatsis2\": \"Second AIATSIS code\",\n",
    "            \"aiatsisname2\": \"Second AIATSIS name\",\n",
    "            \"guthrie\": \"Guthrie code (Bantu)\",\n",
    "            \"ELP\": \"Endangered Languages Project\",\n",
    "            \"ELPname\": \"ELP name\",\n",
    "            \"ELP2\": \"Second ELP link\",\n",
    "            \"ELPname2\": \"Second ELP name\",\n",
    "            \"glottopedia\": \"Glottopedia code\",\n",
    "            \"lingua\": \"Linguasphere code\",\n",
    "            \"lingua_ref\": \"Linguasphere reference\",\n",
    "            \"ietf\": \"IETF language tag\",\n",
    "            \n",
    "            # Media\n",
    "            \"image\": \"Image file name\",\n",
    "            \"imagescale\": \"Image scale\",\n",
    "            \"imagealt\": \"Image alt text\",\n",
    "            \"imagecaption\": \"Image caption\",\n",
    "            \"imageheader\": \"Image header\",\n",
    "            \"map\": \"Map file name\",\n",
    "            \"mapscale\": \"Map scale\",\n",
    "            \"mapalt\": \"Map alt text\",\n",
    "            \"mapcaption\": \"Map caption\",\n",
    "            \"map2\": \"Second map\",\n",
    "            \"mapalt2\": \"Second map alt text\",\n",
    "            \"mapcaption2\": \"Second map caption\",\n",
    "            \"pushpin_map\": \"Pushpin map\",\n",
    "            \"pushpin_image\": \"Pushpin image\",\n",
    "            \"pushpin_map_alt\": \"Pushpin map alt text\",\n",
    "            \"pushpin_map_caption\": \"Pushpin map caption\",\n",
    "            \"pushpin_mapsize\": \"Pushpin map size\",\n",
    "            \"pushpin_label\": \"Pushpin label\",\n",
    "            \"pushpin_label_position\": \"Pushpin label position\",\n",
    "            \"coordinates\": \"Geographic coordinates\",\n",
    "            \n",
    "            # Constructed languages\n",
    "            \"creator\": \"Language creator\",\n",
    "            \"created\": \"Year created\",\n",
    "            \"setting\": \"Usage setting\",\n",
    "            \n",
    "            # Speaker information\n",
    "            \"speakers_label\": \"Speaker label override\",\n",
    "            \"refname\": \"Reference name\",\n",
    "            \n",
    "            # Formatting\n",
    "            \"boxsize\": \"Infobox width override\",\n",
    "            \"fontcolor\": \"Font color override\",\n",
    "            \"module\": \"Embedded module\",\n",
    "            \"notice\": \"Footer notice\"\n",
    "        }\n",
    "\n",
    "# Convenience functions for easy usage\n",
    "\n",
    "def extract_infobox_by_qid(qid: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract Wikipedia language infobox using QID.\n",
    "    \n",
    "    Args:\n",
    "        qid (str): Wikidata QID (e.g., 'Q1860' for English)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with infobox fields and metadata\n",
    "        \n",
    "    Example:\n",
    "        >>> result = extract_infobox_by_qid('Q1860')\n",
    "        >>> print(result['fields']['name'])  # 'English'\n",
    "        >>> print(result['fields']['iso1'])  # 'en'\n",
    "    \"\"\"\n",
    "    extractor = WikipediaInfoboxExtractor()\n",
    "    return extractor.extract_infobox_by_qid(qid)\n",
    "\n",
    "def extract_infobox_by_title(page_title: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract Wikipedia language infobox by page title.\n",
    "    \n",
    "    Args:\n",
    "        page_title (str): Wikipedia page title (e.g., 'English language')\n",
    "        \n",
    "    Returns:\n",
    "        Dict with infobox fields and metadata\n",
    "        \n",
    "    Example:\n",
    "        >>> result = extract_infobox_by_title('French language')\n",
    "        >>> print(result['fields']['name'])  # 'French'\n",
    "        >>> print(result['fields']['nativename'])  # 'français'\n",
    "    \"\"\"\n",
    "    extractor = WikipediaInfoboxExtractor()\n",
    "    return extractor.extract_infobox_by_title(page_title)\n",
    "\n",
    "def get_supported_fields() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get list of supported infobox fields with descriptions.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping field names to descriptions\n",
    "        \n",
    "    Example:\n",
    "        >>> fields = get_supported_fields()\n",
    "        >>> print(fields['name'])  # 'Language name'\n",
    "        >>> print(fields['speakers'])  # 'Number of native speakers'\n",
    "    \"\"\"\n",
    "    extractor = WikipediaInfoboxExtractor()\n",
    "    return extractor.get_infobox_template_fields()\n",
    "\n",
    "def search_language_pages(search_term: str, limit: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Search for language pages on Wikipedia.\n",
    "    \n",
    "    Args:\n",
    "        search_term (str): Search term (e.g., 'French language')\n",
    "        limit (int): Maximum number of results\n",
    "        \n",
    "    Returns:\n",
    "        List of page titles\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            'action': 'opensearch',\n",
    "            'format': 'json',\n",
    "            'search': search_term + ' language',\n",
    "            'limit': limit,\n",
    "            'namespace': 0,\n",
    "            'suggest': True\n",
    "        }\n",
    "        \n",
    "        response = requests.get(api_url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        \n",
    "        if len(data) >= 2:\n",
    "            return data[1]  # Return the list of titles\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Testing function\n",
    "def test_infobox_extraction():\n",
    "    \"\"\"Test infobox extraction with various languages.\"\"\"\n",
    "    test_cases = [\n",
    "        ('Q1860', 'English'),\n",
    "        ('Q150', 'French'),\n",
    "        ('Q188', 'German'),\n",
    "        ('Q1321', 'Spanish'),\n",
    "        ('Q7737', 'Russian')\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for qid, lang_name in test_cases:\n",
    "        print(f\"\\\\nTesting {lang_name} ({qid})...\")\n",
    "        try:\n",
    "            result = extract_infobox_by_qid(qid)\n",
    "            if 'error' not in result:\n",
    "                field_count = result.get('field_count', 0)\n",
    "                print(f\"✓ Extracted {field_count} infobox fields\")\n",
    "                if 'fields' in result and 'name' in result['fields']:\n",
    "                    print(f\"  Language name: {result['fields']['name']}\")\n",
    "                if 'fields' in result and 'speakers' in result['fields']:\n",
    "                    print(f\"  Speakers: {result['fields']['speakers']}\")\n",
    "            else:\n",
    "                print(f\"✗ Error: {result['error']}\")\n",
    "            results[qid] = result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Exception: {e}\")\n",
    "            results[qid] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Wikipedia Language Infobox Extractor\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example: Extract English language infobox\n",
    "    print(\"\\\\nExtracting English language infobox (Q1860)...\")\n",
    "    result = extract_infobox_by_qid('Q1860')\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"✓ Successfully extracted infobox from: {result['page_title']}\")\n",
    "        print(f\"✓ Found {result['field_count']} fields\")\n",
    "        print(\"\\\\nSample fields:\")\n",
    "        for key, value in list(result['fields'].items())[:8]:\n",
    "            # Truncate long values for display\n",
    "            display_value = value[:100] + \"...\" if len(value) > 100 else value\n",
    "            print(f\"  {key}: {display_value}\")\n",
    "        \n",
    "        print(f\"\\\\n✓ Wikipedia URL: {result['wikipedia_url']}\")\n",
    "    else:\n",
    "        print(f\"✗ Error: {result['error']}\")\n",
    "    \n",
    "    # Show some supported fields\n",
    "    print(\"\\\\n\" + \"=\" * 50)\n",
    "    print(\"Some supported Template:Infobox language fields:\")\n",
    "    fields = get_supported_fields()\n",
    "    important_fields = ['name', 'nativename', 'speakers', 'family', 'script', 'iso1', 'iso3']\n",
    "    for field in important_fields:\n",
    "        if field in fields:\n",
    "            print(f\"  {field}: {fields[field]}\")\n",
    "    print(f\"\\\\n... and {len(fields) - len(important_fields)} more fields supported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
