{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd63b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fetching relationships for: English ---\n",
      "\n",
      "1. Querying Wikidata...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "   Found 6 dialects via wikitext parser.\n",
      "\n",
      "--- Combined Results ---\n",
      "{\n",
      "  \"language\": \"English\",\n",
      "  \"parents\": [\n",
      "    \"Anglic\"\n",
      "  ],\n",
      "  \"children\": [\n",
      "    \"African Nova Scotian English\",\n",
      "    \"American Indian English\",\n",
      "    \"Bahamian English\",\n",
      "    \"Bajan English\",\n",
      "    \"Basic English\",\n",
      "    \"Bay Islands English\",\n",
      "    \"British English\",\n",
      "    \"Brittonicisms in English\",\n",
      "    \"Brunei English\",\n",
      "    \"Buddhist Hybrid English\",\n",
      "    \"Butler English\",\n",
      "    \"Cameroonian English\",\n",
      "    \"Caribbean English\",\n",
      "    \"Chancery Standard\",\n",
      "    \"Channel Island English\",\n",
      "    \"Cyrillic English\",\n",
      "    \"Degawa English\",\n",
      "    \"Denglisch\",\n",
      "    \"Ebonics\",\n",
      "    \"English as a second or foreign language\",\n",
      "    \"English in the Netherlands\",\n",
      "    \"English language in Algeria\",\n",
      "    \"English language in Europe\",\n",
      "    \"English language in Lebanon\",\n",
      "    \"English language in Puerto Rico\",\n",
      "    \"English language in Spain\",\n",
      "    \"English language in Sweden\",\n",
      "    \"English language in Switzerland\",\n",
      "    \"English language in Ukraine\",\n",
      "    \"Euro English\",\n",
      "    \"Falkland Islands English\",\n",
      "    \"Gambian English\",\n",
      "    \"Ghanaian English\",\n",
      "    \"Ghanaian Pidgin English\",\n",
      "    \"Gibraltarian English\",\n",
      "    \"Globish\",\n",
      "    \"Hawaiian English\",\n",
      "    \"Hebronics\",\n",
      "    \"Hiberno-English\",\n",
      "    \"Hong Kong English\",\n",
      "    \"International English\",\n",
      "    \"Iyaric\",\n",
      "    \"Jamaican English\",\n",
      "    \"Jewish English varieties\",\n",
      "    \"Kenyan English\",\n",
      "    \"Legal English\",\n",
      "    \"Luminar English\",\n",
      "    \"Malawian English\",\n",
      "    \"Maltese English\",\n",
      "    \"Manx English\",\n",
      "    \"Middle English\",\n",
      "    \"Namlish\",\n",
      "    \"New Zealand English\",\n",
      "    \"Newfoundland English\",\n",
      "    \"Nigerian English\",\n",
      "    \"North American English\",\n",
      "    \"Northern American English\",\n",
      "    \"Ozark English\",\n",
      "    \"Palauan English\",\n",
      "    \"Pennsylvania Dutch English\",\n",
      "    \"Philippine English\",\n",
      "    \"Pig Latin\",\n",
      "    \"Pirate English\",\n",
      "    \"Q2920188\",\n",
      "    \"Q3511588\",\n",
      "    \"Saint Helena English\",\n",
      "    \"Scottish English\",\n",
      "    \"Sierra Leonean English\",\n",
      "    \"Simple English\",\n",
      "    \"Simplified Technical English\",\n",
      "    \"Singapore English\",\n",
      "    \"Singlish\",\n",
      "    \"Solomon Islands English\",\n",
      "    \"Solomon Islands Pidgin English\",\n",
      "    \"South African English\",\n",
      "    \"South Asian English\",\n",
      "    \"South Atlantic English\",\n",
      "    \"Southeast Asian English\",\n",
      "    \"Standard English\",\n",
      "    \"Tinglish\",\n",
      "    \"Torres Strait English\",\n",
      "    \"Trinidadian and Tobagonian English\",\n",
      "    \"Tristan da Cunha English\",\n",
      "    \"U and non-U English\",\n",
      "    \"Ugandan English\",\n",
      "    \"Ulster English\",\n",
      "    \"Utah Mormon English\",\n",
      "    \"Zambian English\",\n",
      "    \"Zimbabwean English\",\n",
      "    \"Zulu English\",\n",
      "    \"broken English\",\n",
      "    \"dialects of English\",\n",
      "    \"rhyming slang\",\n",
      "    \"world Englishes\"\n",
      "  ],\n",
      "  \"siblings\": [],\n",
      "  \"dialects\": [\n",
      "    \"Australian English\",\n",
      "    \"Caribbean English\",\n",
      "    \"Hiberno-English\",\n",
      "    \"New Zealand English\",\n",
      "    \"North American English\",\n",
      "    \"South African English\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "==============================\n",
      "\n",
      "   Found 6 dialects via wikitext parser.\n",
      "\n",
      "--- Combined Results ---\n",
      "{\n",
      "  \"language\": \"English\",\n",
      "  \"parents\": [\n",
      "    \"Anglic\"\n",
      "  ],\n",
      "  \"children\": [\n",
      "    \"African Nova Scotian English\",\n",
      "    \"American Indian English\",\n",
      "    \"Bahamian English\",\n",
      "    \"Bajan English\",\n",
      "    \"Basic English\",\n",
      "    \"Bay Islands English\",\n",
      "    \"British English\",\n",
      "    \"Brittonicisms in English\",\n",
      "    \"Brunei English\",\n",
      "    \"Buddhist Hybrid English\",\n",
      "    \"Butler English\",\n",
      "    \"Cameroonian English\",\n",
      "    \"Caribbean English\",\n",
      "    \"Chancery Standard\",\n",
      "    \"Channel Island English\",\n",
      "    \"Cyrillic English\",\n",
      "    \"Degawa English\",\n",
      "    \"Denglisch\",\n",
      "    \"Ebonics\",\n",
      "    \"English as a second or foreign language\",\n",
      "    \"English in the Netherlands\",\n",
      "    \"English language in Algeria\",\n",
      "    \"English language in Europe\",\n",
      "    \"English language in Lebanon\",\n",
      "    \"English language in Puerto Rico\",\n",
      "    \"English language in Spain\",\n",
      "    \"English language in Sweden\",\n",
      "    \"English language in Switzerland\",\n",
      "    \"English language in Ukraine\",\n",
      "    \"Euro English\",\n",
      "    \"Falkland Islands English\",\n",
      "    \"Gambian English\",\n",
      "    \"Ghanaian English\",\n",
      "    \"Ghanaian Pidgin English\",\n",
      "    \"Gibraltarian English\",\n",
      "    \"Globish\",\n",
      "    \"Hawaiian English\",\n",
      "    \"Hebronics\",\n",
      "    \"Hiberno-English\",\n",
      "    \"Hong Kong English\",\n",
      "    \"International English\",\n",
      "    \"Iyaric\",\n",
      "    \"Jamaican English\",\n",
      "    \"Jewish English varieties\",\n",
      "    \"Kenyan English\",\n",
      "    \"Legal English\",\n",
      "    \"Luminar English\",\n",
      "    \"Malawian English\",\n",
      "    \"Maltese English\",\n",
      "    \"Manx English\",\n",
      "    \"Middle English\",\n",
      "    \"Namlish\",\n",
      "    \"New Zealand English\",\n",
      "    \"Newfoundland English\",\n",
      "    \"Nigerian English\",\n",
      "    \"North American English\",\n",
      "    \"Northern American English\",\n",
      "    \"Ozark English\",\n",
      "    \"Palauan English\",\n",
      "    \"Pennsylvania Dutch English\",\n",
      "    \"Philippine English\",\n",
      "    \"Pig Latin\",\n",
      "    \"Pirate English\",\n",
      "    \"Q2920188\",\n",
      "    \"Q3511588\",\n",
      "    \"Saint Helena English\",\n",
      "    \"Scottish English\",\n",
      "    \"Sierra Leonean English\",\n",
      "    \"Simple English\",\n",
      "    \"Simplified Technical English\",\n",
      "    \"Singapore English\",\n",
      "    \"Singlish\",\n",
      "    \"Solomon Islands English\",\n",
      "    \"Solomon Islands Pidgin English\",\n",
      "    \"South African English\",\n",
      "    \"South Asian English\",\n",
      "    \"South Atlantic English\",\n",
      "    \"Southeast Asian English\",\n",
      "    \"Standard English\",\n",
      "    \"Tinglish\",\n",
      "    \"Torres Strait English\",\n",
      "    \"Trinidadian and Tobagonian English\",\n",
      "    \"Tristan da Cunha English\",\n",
      "    \"U and non-U English\",\n",
      "    \"Ugandan English\",\n",
      "    \"Ulster English\",\n",
      "    \"Utah Mormon English\",\n",
      "    \"Zambian English\",\n",
      "    \"Zimbabwean English\",\n",
      "    \"Zulu English\",\n",
      "    \"broken English\",\n",
      "    \"dialects of English\",\n",
      "    \"rhyming slang\",\n",
      "    \"world Englishes\"\n",
      "  ],\n",
      "  \"siblings\": [],\n",
      "  \"dialects\": [\n",
      "    \"Australian English\",\n",
      "    \"Caribbean English\",\n",
      "    \"Hiberno-English\",\n",
      "    \"New Zealand English\",\n",
      "    \"North American English\",\n",
      "    \"South African English\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import mwparserfromhell\n",
    "import wikipediaapi\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "########################\n",
    "## Wikidata + Wikipedia helpers\n",
    "########################\n",
    "\n",
    "# --- Wikidata query for relationships ---\n",
    "def get_language_relationships_wikidata(language_name):\n",
    "    \"\"\"\n",
    "    Fetch language relationships from Wikidata using a SPARQL query.\n",
    "\n",
    "    Notes:\n",
    "    - Dialects are fetched via property P5019 (\"dialect of\").\n",
    "      We query for items (?dialect) where ?dialect wdt:P5019 ?language.\n",
    "      This avoids location/region pages like \"England\" or \"Ancient Rome\".\n",
    "    \"\"\"\n",
    "    # SPARQL: focus on dialect-of (P5019) and keep simple parent/child placeholders.\n",
    "    sparql_query = f\"\"\"\n",
    "    SELECT ?parentLabel ?childLabel ?dialectLabel ?siblingLabel\n",
    "    WHERE {{\n",
    "      ?language rdfs:label \"{language_name}\"@en .\n",
    "      ?language wdt:P31/wdt:P279* wd:Q34770 .  # instance/subclass of language\n",
    "\n",
    "      # Optional: parent/child relationships (kept simple; may vary per item quality)\n",
    "      OPTIONAL {{ ?language wdt:P279 ?parent . }}  # subclass of (sometimes used for family)\n",
    "      OPTIONAL {{ ?child wdt:P279 ?language . }}  # children as subclasses (rare but possible)\n",
    "\n",
    "      # Dialects: items that are 'dialect of' this language\n",
    "      OPTIONAL {{ ?dialect wdt:P5019 ?language . }}\n",
    "\n",
    "      # Siblings: other dialects of the same parent (if applicable)\n",
    "      OPTIONAL {{\n",
    "        ?sibling wdt:P5019 ?commonParent .\n",
    "        ?language wdt:P5019 ?commonParent .\n",
    "        FILTER(?sibling != ?language)\n",
    "      }}\n",
    "\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'LanguageRelationshipFetcher/1.1 (educational@example.com)'\n",
    "        }\n",
    "        response = requests.get(url, params={'query': sparql_query, 'format': 'json'}, headers=headers, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying Wikidata: {e}\")\n",
    "        return {}\n",
    "\n",
    "    results = {\n",
    "        \"parents\": set(),\n",
    "        \"children\": set(),\n",
    "        \"dialects\": set(),\n",
    "        \"siblings\": set(),\n",
    "    }\n",
    "\n",
    "    for item in data.get('results', {}).get('bindings', []):\n",
    "        if 'parentLabel' in item:\n",
    "            results['parents'].add(item['parentLabel']['value'])\n",
    "        if 'childLabel' in item:\n",
    "            results['children'].add(item['childLabel']['value'])\n",
    "        if 'dialectLabel' in item:\n",
    "            results['dialects'].add(item['dialectLabel']['value'])\n",
    "        if 'siblingLabel' in item:\n",
    "            results['siblings'].add(item['siblingLabel']['value'])\n",
    "\n",
    "    for key in results:\n",
    "        results[key] = sorted(list(results[key]))\n",
    "    return results\n",
    "\n",
    "\n",
    "########################\n",
    "## Wikipedia infobox parsing (mwparserfromhell)\n",
    "########################\n",
    "\n",
    "# --- Original infobox parser using wikipediaapi + mwparserfromhell ---\n",
    "def get_language_relationships_infobox(language_name):\n",
    "    \"\"\"\n",
    "    Fetches language relationships from the Wikipedia infobox.\n",
    "    \"\"\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('LanguageTreeBuilder/1.0 (educational@example.com)', 'en')\n",
    "    page = wiki_wiki.page(language_name)\n",
    "\n",
    "    if not page.exists():\n",
    "        print(f\"Page for '{language_name}' not found on Wikipedia.\")\n",
    "        return {}\n",
    "\n",
    "    wikicode = mwparserfromhell.parse(page.text)\n",
    "\n",
    "    infoboxes = wikicode.filter_templates(matches=lambda t: t.name.strip().lower().startswith('infobox language'))\n",
    "\n",
    "    if not infoboxes:\n",
    "        return {}\n",
    "    infobox = infoboxes[0]\n",
    "\n",
    "    results = {\"parents\": set(), \"dialects\": set()}\n",
    "    parent_params = ['family', 'fam', 'family1', 'fam1', 'ancestor', 'ancestors']\n",
    "    dialect_params = ['dialects', 'varieties']\n",
    "\n",
    "    for param in infobox.params:\n",
    "        param_name = param.name.strip().lower()\n",
    "        param_value = param.value.strip_code().strip()\n",
    "\n",
    "        if any(p in param_name for p in parent_params):\n",
    "            parents = [p.strip() for p in param_value.replace('\\n', ',').split(',') if p.strip()]\n",
    "            results['parents'].update(parents)\n",
    "\n",
    "        if any(d in param_name for d in dialect_params):\n",
    "            dialects = [d.strip() for d in param_value.replace('\\n', ',').split(',') if d.strip()]\n",
    "            results['dialects'].update(dialects)\n",
    "\n",
    "    for key in results:\n",
    "        results[key] = sorted(list(results[key]))\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "## Raw wikitext dialect extractor (dia1..dia40)\n",
    "########################\n",
    "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\"User-Agent\": \"LanguageTreeNotebook/1.0 (educational)\"}\n",
    "\n",
    "\n",
    "def _get_page_content(title: str) -> str:\n",
    "    \"\"\"Fetch raw wikitext content of the given page title.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        if not pages:\n",
    "            return \"\"\n",
    "        page = next(iter(pages.values()))\n",
    "        revs = page.get(\"revisions\")\n",
    "        if not revs:\n",
    "            return \"\"\n",
    "        return revs[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _find_wiki_links(text: str) -> List[str]:\n",
    "    \"\"\"Return list of linked page titles from wiki link markup [[Title|...]].\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    links = []\n",
    "    for m in re.finditer(r\"\\[\\[([^|#\\]]+)(?:\\|[^\\]]*)?\\]\\]\", text):\n",
    "        t = m.group(1).strip()\n",
    "        if t:\n",
    "            links.append(t)\n",
    "    return links\n",
    "\n",
    "\n",
    "def _extract_infobox(wikitext: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract raw key->value pairs from the Infobox (language or language family).\"\"\"\n",
    "    if not wikitext:\n",
    "        return {}\n",
    "\n",
    "    start = wikitext.find(\"{{Infobox language\")\n",
    "    if start == -1:\n",
    "        start = wikitext.find(\"{{Infobox language family\")\n",
    "    if start == -1:\n",
    "        m = re.search(r\"\\{\\{infobox\\s+(language|language family)\", wikitext, re.IGNORECASE)\n",
    "        if m:\n",
    "            start = m.start()\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    # Find the matching closing braces for the infobox\n",
    "    pos = start + 2\n",
    "    depth = 1\n",
    "    end = -1\n",
    "    while pos < len(wikitext):\n",
    "        if wikitext[pos:pos+2] == \"{{\":\n",
    "            depth += 1\n",
    "            pos += 2\n",
    "        elif wikitext[pos:pos+2] == \"}}\":\n",
    "            depth -= 1\n",
    "            pos += 2\n",
    "            if depth == 0:\n",
    "                end = pos\n",
    "                break\n",
    "        else:\n",
    "            pos += 1\n",
    "    if end == -1:\n",
    "        return {}\n",
    "\n",
    "    content = wikitext[start:end]\n",
    "    raw: Dict[str, str] = {}\n",
    "    current_key = None\n",
    "    current_val_lines: List[str] = []\n",
    "\n",
    "    for line in content.split(\"\\n\"):\n",
    "        s = line.strip()\n",
    "        if s.lower().startswith(\"{{infobox language\"):\n",
    "            continue\n",
    "        if s.startswith(\"|\") and \"=\" in s:\n",
    "            if current_key is not None and current_val_lines:\n",
    "                raw[current_key] = \"\\n\".join(current_val_lines).strip()\n",
    "            key, val = s[1:].split(\"=\", 1)\n",
    "            current_key = key.strip()\n",
    "            current_val_lines = [val.strip()]\n",
    "        elif s.startswith(\"|\") and current_key is not None:\n",
    "            current_val_lines.append(s[1:].strip())\n",
    "        elif current_key is not None and not s.startswith(\"|\"):\n",
    "            current_val_lines.append(s)\n",
    "\n",
    "    if current_key is not None and current_val_lines:\n",
    "        raw[current_key] = \"\\n\".join(current_val_lines).strip()\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def _get_page_categories(title: str) -> List[str]:\n",
    "    \"\"\"Fetch non-hidden categories for a Wikipedia page title.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"categories\",\n",
    "        \"titles\": title,\n",
    "        \"clshow\": \"!hidden\",\n",
    "        \"cllimit\": \"50\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        if not pages:\n",
    "            return []\n",
    "        page = next(iter(pages.values()))\n",
    "        cats = page.get(\"categories\", []) or []\n",
    "        return [c.get(\"title\", \"\") for c in cats if c.get(\"title\")]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def _is_probable_geopage(title: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: exclude pages that are likely countries/regions/geography.\n",
    "    Uses categories to spot geographic topics.\n",
    "    \"\"\"\n",
    "    cats = _get_page_categories(title)\n",
    "    if not cats:\n",
    "        # No categories found; don't exclude solely based on that.\n",
    "        return False\n",
    "    joined = \" \".join(cats).lower()\n",
    "    geo_keywords = [\n",
    "        \"country\",\"countries\",\"sovereign state\",\"sovereign states\",\"empire\",\"empires\",\n",
    "        \"kingdom\",\"kingdoms\",\"roman\",\"province\",\"provinces\",\"city\",\"cities\",\n",
    "        \"populated places\",\"regions of\",\"counties of\",\"states of\",\"geography of\",\n",
    "    ]\n",
    "    return any(k in joined for k in geo_keywords)\n",
    "\n",
    "\n",
    "def _filter_dialect_titles(titles: List[str], language_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Remove obvious non-dialect entries such as list pages, media namespaces,\n",
    "    the language itself, and geographic pages (countries/regions).\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    seen = set()\n",
    "    for t in titles:\n",
    "        tt = (t or \"\").strip()\n",
    "        if not tt:\n",
    "            continue\n",
    "        low = tt.lower()\n",
    "        # Exclude list/maintenance and non-article namespaces\n",
    "        if low.startswith(\"list of\") or low.startswith(\"outline of\") or low.startswith(\"history of\"):\n",
    "            continue\n",
    "        if any(tt.startswith(ns) for ns in (\"Category:\", \"File:\", \"Image:\", \"Template:\")):\n",
    "            continue\n",
    "        # Exclude the language's main page variants\n",
    "        if tt in {language_name, f\"{language_name} language\", f\"{language_name} Language\"}:\n",
    "            continue\n",
    "        # Exclude likely geo pages (countries/regions)\n",
    "        if _is_probable_geopage(tt):\n",
    "            continue\n",
    "        if tt not in seen:\n",
    "            seen.add(tt)\n",
    "            out.append(tt)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_dialect_relationships(language_name: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Return only (dialect, 'dialect_of', language_name) tuples extracted from the\n",
    "    language's Wikipedia infobox. Looks at 'dialects' and 'dia1'..'dia40' fields,\n",
    "    and filters out non-dialect pages (e.g., countries, lists).\n",
    "    \"\"\"\n",
    "    if not language_name or not isinstance(language_name, str):\n",
    "        return []\n",
    "\n",
    "    # Try a few common page title variations\n",
    "    candidates = [\n",
    "        f\"{language_name} language\",\n",
    "        language_name,\n",
    "        f\"{language_name} Language\",\n",
    "        f\"{language_name} languages\",\n",
    "        f\"{language_name} language family\",\n",
    "    ]\n",
    "\n",
    "    wikitext = \"\"\n",
    "    for t in candidates:\n",
    "        wikitext = _get_page_content(t)\n",
    "        if wikitext and (\"{{Infobox language\" in wikitext or \"{{Infobox language family\" in wikitext):\n",
    "            break\n",
    "    if not wikitext:\n",
    "        return []\n",
    "\n",
    "    infobox_raw = _extract_infobox(wikitext)\n",
    "    if not infobox_raw:\n",
    "        return []\n",
    "\n",
    "    found: List[str] = []\n",
    "\n",
    "    # Collect from explicit 'dialects' field if it contains links\n",
    "    if \"dialects\" in infobox_raw:\n",
    "        found.extend(_find_wiki_links(infobox_raw[\"dialects\"]))\n",
    "\n",
    "    # Collect from dia1..dia40 fields\n",
    "    for i in range(1, 41):\n",
    "        k = f\"dia{i}\"\n",
    "        if k in infobox_raw:\n",
    "            found.extend(_find_wiki_links(infobox_raw[k]))\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    ordered: List[str] = []\n",
    "    for d in found:\n",
    "        if d not in seen:\n",
    "            seen.add(d)\n",
    "            ordered.append(d)\n",
    "\n",
    "    # Filter out non-dialect pages\n",
    "    dialects = _filter_dialect_titles(ordered, language_name)\n",
    "\n",
    "    return [(d, \"dialect_of\", language_name) for d in dialects]\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "## Pipeline\n",
    "########################\n",
    "def get_language_relationships(language_name):\n",
    "    \"\"\"\n",
    "    Main pipeline function to get language relationships.\n",
    "    \"\"\"\n",
    "    print(f\"--- Fetching relationships for: {language_name} ---\\n\")\n",
    "\n",
    "    print(\"1. Querying Wikidata...\")\n",
    "    wikidata_results = get_language_relationships_wikidata(language_name)\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "    print(\"2. Parsing Wikipedia Infobox (mwparserfromhell)...\")\n",
    "    infobox_results = get_language_relationships_infobox(language_name)\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "    print(\"3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\")\n",
    "    dialect_tuples = get_dialect_relationships(language_name)\n",
    "    wikitext_dialects = [d for (d, _rel, _lang) in dialect_tuples]\n",
    "    print(f\"   Found {len(wikitext_dialects)} dialects via wikitext parser.\\n\")\n",
    "\n",
    "    combined_parents = set(wikidata_results.get(\"parents\", []))\n",
    "    if \"parents\" in infobox_results:\n",
    "        combined_parents.update(infobox_results[\"parents\"])\n",
    "\n",
    "    # Filter infobox-derived dialects as well\n",
    "    infobox_dialects = infobox_results.get(\"dialects\", [])\n",
    "    infobox_dialects_filtered = _filter_dialect_titles(infobox_dialects, language_name)\n",
    "\n",
    "    combined_dialects = set(wikidata_results.get(\"dialects\", []))\n",
    "    combined_dialects.update(infobox_dialects_filtered)\n",
    "    combined_dialects.update(wikitext_dialects)\n",
    "\n",
    "    return {\n",
    "        \"language\": language_name,\n",
    "        \"parents\": sorted(list(combined_parents)),\n",
    "        \"children\": wikidata_results.get(\"children\", []),\n",
    "        \"siblings\": wikidata_results.get(\"siblings\", []),\n",
    "        \"dialects\": sorted(list(combined_dialects)),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    language_to_check = \"English\"\n",
    "    relationships_data = get_language_relationships(language_to_check)\n",
    "    print(\"--- Combined Results ---\")\n",
    "    print(json.dumps(relationships_data, indent=2))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c8ec2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Latin ===\n",
      "--- Fetching relationships for: Latin ---\n",
      "\n",
      "1. Querying Wikidata...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "   Found 0 dialects via wikitext parser.\n",
      "\n",
      "Dialects: []\n",
      "\n",
      "=== English ===\n",
      "--- Fetching relationships for: English ---\n",
      "\n",
      "1. Querying Wikidata...\n",
      "   Found 0 dialects via wikitext parser.\n",
      "\n",
      "Dialects: []\n",
      "\n",
      "=== English ===\n",
      "--- Fetching relationships for: English ---\n",
      "\n",
      "1. Querying Wikidata...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "   Found 6 dialects via wikitext parser.\n",
      "\n",
      "Dialects: ['Australian English', 'Caribbean English', 'Hiberno-English', 'New Zealand English', 'North American English', 'South African English']\n",
      "   Found 6 dialects via wikitext parser.\n",
      "\n",
      "Dialects: ['Australian English', 'Caribbean English', 'Hiberno-English', 'New Zealand English', 'North American English', 'South African English']\n"
     ]
    }
   ],
   "source": [
    "# Quick checks\n",
    "for lang in [\"Latin\", \"English\"]:\n",
    "    print(\"\\n===\", lang, \"===\")\n",
    "    data = get_language_relationships(lang)\n",
    "    print(\"Dialects:\", data.get(\"dialects\", [])[:25])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
