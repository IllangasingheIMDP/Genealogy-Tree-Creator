{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd63b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fetching relationships for: Latin ---\n",
      "\n",
      "1. Querying Wikidata...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "2. Parsing Wikipedia Infobox (mwparserfromhell)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "Done.\n",
      "\n",
      "3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\n",
      "   Found 0 dialects via wikitext parser.\n",
      "\n",
      "--- Combined Results ---\n",
      "{\n",
      "  \"language\": \"Latin\",\n",
      "  \"parents\": [\n",
      "    \"Latino-Faliscan\",\n",
      "    \"Southern European language\"\n",
      "  ],\n",
      "  \"children\": [],\n",
      "  \"siblings\": [],\n",
      "  \"dialects\": [\n",
      "    \"Ancient Rome\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "==============================\n",
      "\n",
      "   Found 0 dialects via wikitext parser.\n",
      "\n",
      "--- Combined Results ---\n",
      "{\n",
      "  \"language\": \"Latin\",\n",
      "  \"parents\": [\n",
      "    \"Latino-Faliscan\",\n",
      "    \"Southern European language\"\n",
      "  ],\n",
      "  \"children\": [],\n",
      "  \"siblings\": [],\n",
      "  \"dialects\": [\n",
      "    \"Ancient Rome\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import mwparserfromhell\n",
    "import wikipediaapi\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "# --- Wikidata query for relationships ---\n",
    "def get_language_relationships_wikidata(language_name):\n",
    "    \"\"\"\n",
    "    Fetches language relationships from Wikidata using a SPARQL query.\n",
    "    \"\"\"\n",
    "    sparql_query = f\"\"\"\n",
    "    SELECT ?parentLabel ?childLabel ?dialectLabel ?siblingLabel\n",
    "    WHERE {{\n",
    "      ?language rdfs:label \"{language_name}\"@en .\n",
    "      ?language wdt:P31/wdt:P279* wd:Q34770 .\n",
    "\n",
    "      OPTIONAL {{ ?language wdt:P279 ?parent . }}\n",
    "      OPTIONAL {{ ?language wdt:P155 ?parent . }}\n",
    "      OPTIONAL {{ ?language wdt:P220 ?parent . }}\n",
    "      OPTIONAL {{ ?child wdt:P155 ?language . BIND(?language as ?parent) }}\n",
    "\n",
    "      OPTIONAL {{ ?child wdt:P155 ?language . }}\n",
    "\n",
    "      OPTIONAL {{ ?language wdt:P2341 ?dialect . }}\n",
    "      OPTIONAL {{ ?dialect wdt:P629 ?language . }}\n",
    "\n",
    "      OPTIONAL {{\n",
    "        ?language wdt:P155 ?commonParent .\n",
    "        ?sibling wdt:P155 ?commonParent .\n",
    "        FILTER(?sibling != ?language)\n",
    "      }}\n",
    "\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'LanguageRelationshipFetcher/1.0 (educational@example.com)'\n",
    "        }\n",
    "        response = requests.get(url, params={'query': sparql_query, 'format': 'json'}, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying Wikidata: {e}\")\n",
    "        return {}\n",
    "\n",
    "    results = {\n",
    "        \"parents\": set(),\n",
    "        \"children\": set(),\n",
    "        \"dialects\": set(),\n",
    "        \"siblings\": set()\n",
    "    }\n",
    "\n",
    "    for item in data.get('results', {}).get('bindings', []):\n",
    "        if 'parentLabel' in item:\n",
    "            results['parents'].add(item['parentLabel']['value'])\n",
    "        if 'childLabel' in item:\n",
    "            results['children'].add(item['childLabel']['value'])\n",
    "        if 'dialectLabel' in item:\n",
    "            results['dialects'].add(item['dialectLabel']['value'])\n",
    "        if 'siblingLabel' in item:\n",
    "            results['siblings'].add(item['siblingLabel']['value'])\n",
    "\n",
    "    for key in results:\n",
    "        results[key] = sorted(list(results[key]))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Original infobox parser using wikipediaapi + mwparserfromhell ---\n",
    "def get_language_relationships_infobox(language_name):\n",
    "    \"\"\"\n",
    "    Fetches language relationships from the Wikipedia infobox.\n",
    "    \"\"\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('LanguageTreeBuilder/1.0 (educational@example.com)', 'en')\n",
    "    page = wiki_wiki.page(language_name)\n",
    "\n",
    "    if not page.exists():\n",
    "        print(f\"Page for '{language_name}' not found on Wikipedia.\")\n",
    "        return {}\n",
    "\n",
    "    wikicode = mwparserfromhell.parse(page.text)\n",
    "\n",
    "    infoboxes = wikicode.filter_templates(matches=lambda t: t.name.strip().lower().startswith('infobox language'))\n",
    "\n",
    "    if not infoboxes:\n",
    "        return {}\n",
    "    infobox = infoboxes[0]\n",
    "\n",
    "    results = {\"parents\": set(), \"dialects\": set()}\n",
    "    parent_params = ['family', 'fam', 'family1', 'fam1', 'ancestor', 'ancestors']\n",
    "    dialect_params = ['dialects', 'varieties']\n",
    "\n",
    "    for param in infobox.params:\n",
    "        param_name = param.name.strip().lower()\n",
    "        param_value = param.value.strip_code().strip()\n",
    "\n",
    "        if any(p in param_name for p in parent_params):\n",
    "            parents = [p.strip() for p in param_value.replace('\\n', ',').split(',') if p.strip()]\n",
    "            results['parents'].update(parents)\n",
    "\n",
    "        if any(d in param_name for d in dialect_params):\n",
    "            dialects = [d.strip() for d in param_value.replace('\\n', ',').split(',') if d.strip()]\n",
    "            results['dialects'].update(dialects)\n",
    "\n",
    "    for key in results:\n",
    "        results[key] = sorted(list(results[key]))\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- New: Dialect-only extractor using raw Wikipedia wikitext (handles dia1..dia40) ---\n",
    "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\"User-Agent\": \"LanguageTreeNotebook/1.0 (educational)\"}\n",
    "\n",
    "\n",
    "def _get_page_content(title: str) -> str:\n",
    "    \"\"\"Fetch raw wikitext content of the given page title.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        if not pages:\n",
    "            return \"\"\n",
    "        page = next(iter(pages.values()))\n",
    "        revs = page.get(\"revisions\")\n",
    "        if not revs:\n",
    "            return \"\"\n",
    "        return revs[0].get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _find_wiki_links(text: str) -> List[str]:\n",
    "    \"\"\"Return list of linked page titles from wiki link markup [[Title|...]].\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    links = []\n",
    "    for m in re.finditer(r\"\\[\\[([^|#\\]]+)(?:\\|[^\\]]*)?\\]\\]\", text):\n",
    "        t = m.group(1).strip()\n",
    "        if t:\n",
    "            links.append(t)\n",
    "    return links\n",
    "\n",
    "\n",
    "def _extract_infobox(wikitext: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract raw key->value pairs from the Infobox (language or language family).\"\"\"\n",
    "    if not wikitext:\n",
    "        return {}\n",
    "\n",
    "    start = wikitext.find(\"{{Infobox language\")\n",
    "    if start == -1:\n",
    "        start = wikitext.find(\"{{Infobox language family\")\n",
    "    if start == -1:\n",
    "        m = re.search(r\"\\{\\{infobox\\s+(language|language family)\", wikitext, re.IGNORECASE)\n",
    "        if m:\n",
    "            start = m.start()\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    # Find the matching closing braces for the infobox\n",
    "    pos = start + 2\n",
    "    depth = 1\n",
    "    end = -1\n",
    "    while pos < len(wikitext):\n",
    "        if wikitext[pos:pos+2] == \"{{\":\n",
    "            depth += 1\n",
    "            pos += 2\n",
    "        elif wikitext[pos:pos+2] == \"}}\":\n",
    "            depth -= 1\n",
    "            pos += 2\n",
    "            if depth == 0:\n",
    "                end = pos\n",
    "                break\n",
    "        else:\n",
    "            pos += 1\n",
    "    if end == -1:\n",
    "        return {}\n",
    "\n",
    "    content = wikitext[start:end]\n",
    "    raw: Dict[str, str] = {}\n",
    "    current_key = None\n",
    "    current_val_lines: List[str] = []\n",
    "\n",
    "    for line in content.split(\"\\n\"):\n",
    "        s = line.strip()\n",
    "        if s.lower().startswith(\"{{infobox language\"):\n",
    "            continue\n",
    "        if s.startswith(\"|\") and \"=\" in s:\n",
    "            if current_key is not None and current_val_lines:\n",
    "                raw[current_key] = \"\\n\".join(current_val_lines).strip()\n",
    "            key, val = s[1:].split(\"=\", 1)\n",
    "            current_key = key.strip()\n",
    "            current_val_lines = [val.strip()]\n",
    "        elif s.startswith(\"|\") and current_key is not None:\n",
    "            current_val_lines.append(s[1:].strip())\n",
    "        elif current_key is not None and not s.startswith(\"|\"):\n",
    "            current_val_lines.append(s)\n",
    "\n",
    "    if current_key is not None and current_val_lines:\n",
    "        raw[current_key] = \"\\n\".join(current_val_lines).strip()\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def get_dialect_relationships(language_name: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Return only (dialect, 'dialect_of', language_name) tuples extracted from the\n",
    "    language's Wikipedia infobox. Looks at 'dialects' and 'dia1'..'dia40' fields.\n",
    "    \"\"\"\n",
    "    if not language_name or not isinstance(language_name, str):\n",
    "        return []\n",
    "\n",
    "    # Try a few common page title variations\n",
    "    candidates = [\n",
    "        f\"{language_name} language\",\n",
    "        language_name,\n",
    "        f\"{language_name} Language\",\n",
    "        f\"{language_name} languages\",\n",
    "        f\"{language_name} language family\",\n",
    "    ]\n",
    "\n",
    "    wikitext = \"\"\n",
    "    for t in candidates:\n",
    "        wikitext = _get_page_content(t)\n",
    "        if wikitext and (\"{{Infobox language\" in wikitext or \"{{Infobox language family\" in wikitext):\n",
    "            break\n",
    "    if not wikitext:\n",
    "        return []\n",
    "\n",
    "    infobox_raw = _extract_infobox(wikitext)\n",
    "    if not infobox_raw:\n",
    "        return []\n",
    "\n",
    "    found: List[str] = []\n",
    "\n",
    "    # Collect from explicit 'dialects' field if it contains links\n",
    "    if \"dialects\" in infobox_raw:\n",
    "        found.extend(_find_wiki_links(infobox_raw[\"dialects\"]))\n",
    "\n",
    "    # Collect from dia1..dia40 fields\n",
    "    for i in range(1, 41):\n",
    "        k = f\"dia{i}\"\n",
    "        if k in infobox_raw:\n",
    "            found.extend(_find_wiki_links(infobox_raw[k]))\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    dialects: List[str] = []\n",
    "    for d in found:\n",
    "        if d not in seen:\n",
    "            seen.add(d)\n",
    "            dialects.append(d)\n",
    "\n",
    "    return [(d, \"dialect_of\", language_name) for d in dialects]\n",
    "\n",
    "\n",
    "# --- Pipeline ---\n",
    "def get_language_relationships(language_name):\n",
    "    \"\"\"\n",
    "    Main pipeline function to get language relationships.\n",
    "    \"\"\"\n",
    "    print(f\"--- Fetching relationships for: {language_name} ---\\n\")\n",
    "\n",
    "    print(\"1. Querying Wikidata...\")\n",
    "    wikidata_results = get_language_relationships_wikidata(language_name)\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "    print(\"2. Parsing Wikipedia Infobox (mwparserfromhell)...\")\n",
    "    infobox_results = get_language_relationships_infobox(language_name)\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "    print(\"3. Extracting dialects from raw Infobox wikitext (dia1..dia40)...\")\n",
    "    dialect_tuples = get_dialect_relationships(language_name)\n",
    "    wikitext_dialects = [d for (d, _rel, _lang) in dialect_tuples]\n",
    "    print(f\"   Found {len(wikitext_dialects)} dialects via wikitext parser.\\n\")\n",
    "\n",
    "    combined_parents = set(wikidata_results.get(\"parents\", []))\n",
    "    if \"parents\" in infobox_results:\n",
    "        combined_parents.update(infobox_results[\"parents\"])\n",
    "\n",
    "    combined_dialects = set(wikidata_results.get(\"dialects\", []))\n",
    "    if \"dialects\" in infobox_results:\n",
    "        combined_dialects.update(infobox_results[\"dialects\"])\n",
    "    combined_dialects.update(wikitext_dialects)\n",
    "\n",
    "    return {\n",
    "        \"language\": language_name,\n",
    "        \"parents\": sorted(list(combined_parents)),\n",
    "        \"children\": wikidata_results.get(\"children\", []),\n",
    "        \"siblings\": wikidata_results.get(\"siblings\", []),\n",
    "        \"dialects\": sorted(list(combined_dialects)),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    language_to_check = \"Latin\"\n",
    "    relationships_data = get_language_relationships(language_to_check)\n",
    "    print(\"--- Combined Results ---\")\n",
    "    print(json.dumps(relationships_data, indent=2))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ec2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
